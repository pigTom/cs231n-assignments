# Vision Transformer (ViT) å®Œæ•´æ•°æ®æµè¯¦è§£

## ğŸ“‹ ç¤ºä¾‹é…ç½®

```python
# ä½¿ç”¨ CIFAR-10 é…ç½®ä½œä¸ºç¤ºä¾‹
model = VisionTransformer(
    img_size=32,           # å›¾åƒå¤§å°
    patch_size=8,          # Patch å¤§å°
    in_channels=3,         # RGB å›¾åƒ
    embed_dim=128,         # åµŒå…¥ç»´åº¦
    num_layers=6,          # Encoder å±‚æ•°
    num_heads=4,           # æ³¨æ„åŠ›å¤´æ•°
    dim_feedforward=512,   # FFN ä¸­é—´ç»´åº¦
    num_classes=10,        # CIFAR-10 ç±»åˆ«æ•°
    dropout=0.1
)

# è¾“å…¥ï¼šä¸€ä¸ª batch çš„å›¾åƒ
x = torch.randn(2, 3, 32, 32)  # (N=2, C=3, H=32, W=32)
```

**å…³é”®ç»´åº¦**ï¼š
- **N** = 2 (batch size)
- **C** = 3 (RGB channels)
- **H, W** = 32 (image height/width)
- **P** = 8 (patch size)
- **D** = 128 (embed_dim)
- **num_patches** = (32/8)Â² = 16
- **num_heads** = 4
- **head_dim** = 128/4 = 32

---

## ğŸ¬ å®Œæ•´æ•°æ®æµæ¦‚è§ˆ

```
è¾“å…¥å›¾åƒ (N, 3, 32, 32)
    â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Stage 1: Patch Embedding                                  â•‘
â•‘   å°†å›¾åƒåˆ†å‰²æˆ patches å¹¶æŠ•å½±åˆ°åµŒå…¥ç©ºé—´                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â†“ (N, 16, 128)
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Stage 2: Positional Encoding                              â•‘
â•‘   æ·»åŠ ä½ç½®ä¿¡æ¯                                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â†“ (N, 16, 128)
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Stage 3: Transformer Encoder (Ã—6 layers)                  â•‘
â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â•‘
â•‘   â”‚ Layer 1: Self-Attention + FFN           â”‚             â•‘
â•‘   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â•‘
â•‘   â”‚ Layer 2: Self-Attention + FFN           â”‚             â•‘
â•‘   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â•‘
â•‘   â”‚            ...                          â”‚             â•‘
â•‘   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â•‘
â•‘   â”‚ Layer 6: Self-Attention + FFN           â”‚             â•‘
â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â†“ (N, 16, 128)
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Stage 4: Global Average Pooling                           â•‘
â•‘   èšåˆæ‰€æœ‰ patch çš„ä¿¡æ¯                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â†“ (N, 128)
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Stage 5: Classification Head                              â•‘
â•‘   æ˜ å°„åˆ°ç±»åˆ«åˆ†æ•°                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â†“ (N, 10)
è¾“å‡º Logits
```

---

## ğŸ“ Stage 1: Patch Embedding è¯¦è§£

### ç›®æ ‡
å°† 2D å›¾åƒè½¬æ¢ä¸º 1D åºåˆ—çš„ patch embeddingsã€‚

### å®Œæ•´æ­¥éª¤

```
è¾“å…¥: (N, C, H, W) = (2, 3, 32, 32)
ç›®æ ‡: (N, num_patches, embed_dim) = (2, 16, 128)
```

#### Step 1.1: åˆ†å‰²æˆ Patches

```
è¾“å…¥å½¢çŠ¶: (N, C, H, W) = (2, 3, 32, 32)
                 â†“
reshape(N, C, H/P, P, W/P, P)
                 â†“
è¾“å‡ºå½¢çŠ¶: (2, 3, 4, 8, 4, 8)
          â†‘  â†‘  â†‘  â†‘  â†‘  â†‘
          â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ patch å®½åº¦ = 8
          â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€ æ¨ªå‘ patch æ•° = 4
          â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€ patch é«˜åº¦ = 8
          â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ çºµå‘ patch æ•° = 4
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RGB é€šé“ = 3
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ batch size = 2
```

**å¯è§†åŒ–ï¼š**
```
åŸå§‹å›¾åƒ (32Ã—32):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  0  â”‚  1  â”‚  2  â”‚  3  â”‚         â”‚ â† æ¯ä¸ªæ–¹å—æ˜¯ 8Ã—8 patch
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤         â”‚
â”‚  4  â”‚  5  â”‚  6  â”‚  7  â”‚         â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤         â”‚
â”‚  8  â”‚  9  â”‚ 10  â”‚ 11  â”‚         â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤         â”‚
â”‚ 12  â”‚ 13  â”‚ 14  â”‚ 15  â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         4Ã—4 grid = 16 patches
```

**reshape è¿‡ç¨‹ï¼š**
```
(2, 3, 32, 32)
    â†“ å°† H=32 åˆ†æˆ (4ç»„, æ¯ç»„8ä¸ª)
    â†“ å°† W=32 åˆ†æˆ (4ç»„, æ¯ç»„8ä¸ª)
(2, 3, 4, 8, 4, 8)

è§£é‡Šï¼š
- ç»´åº¦ 2: çºµå‘æœ‰ 4 ä¸ª patch
- ç»´åº¦ 3: æ¯ä¸ª patch é«˜åº¦ 8
- ç»´åº¦ 4: æ¨ªå‘æœ‰ 4 ä¸ª patch
- ç»´åº¦ 5: æ¯ä¸ª patch å®½åº¦ 8
```

#### Step 1.2: é‡æ’ç»´åº¦

```
è¾“å…¥: (N, C, num_h, P, num_w, P) = (2, 3, 4, 8, 4, 8)
           â†“ permute(0, 2, 4, 1, 3, 5)
è¾“å‡º: (N, num_h, num_w, C, P, P) = (2, 4, 4, 3, 8, 8)
           â†‘    â†‘      â†‘    â†‘  â†‘  â†‘
           â”‚    â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€ ç©ºé—´ä½ç½®
           â”‚               â””â”€â”€â”€â”€â”€â”€â”€ patch å†…å®¹
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ batch
```

**é‡æ’çš„ç›®çš„ï¼š**
```
åŸå§‹: (N, C, num_h, P, num_w, P)
      é€šé“å’Œç©ºé—´ä¿¡æ¯äº¤é”™

é‡æ’: (N, num_h, num_w, C, P, P)
      â”œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚   ç©ºé—´ç´¢å¼•      patchå†…å®¹
      â”‚   (å“ªä¸ªpatch)   (patchçš„åƒç´ )
      â””â”€ ä¾¿äºåç»­å±•å¹³
```

**å¯è§†åŒ–æ•°æ®ç»„ç»‡ï¼š**
```
é‡æ’åçš„ç»“æ„:
Batch 0:
  Patch (0,0): [3, 8, 8] çš„æ•°æ®  â† å·¦ä¸Šè§’
  Patch (0,1): [3, 8, 8] çš„æ•°æ®
  ...
  Patch (3,3): [3, 8, 8] çš„æ•°æ®  â† å³ä¸‹è§’
Batch 1:
  ...
```

#### Step 1.3: å±•å¹³æ¯ä¸ª Patch

```
è¾“å…¥: (N, num_h, num_w, C, P, P) = (2, 4, 4, 3, 8, 8)
           â†“ reshape(N, num_h*num_w, C*P*P)
è¾“å‡º: (N, num_patches, patch_dim) = (2, 16, 192)
           â†‘      â†‘           â†‘
           â”‚      â”‚           â””â”€ 3Ã—8Ã—8 = 192
           â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  4Ã—4 = 16
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ batch size
```

**å±•å¹³è¿‡ç¨‹ï¼š**
```
æ¯ä¸ª patch:
  (C, P, P) = (3, 8, 8)
      â†“ flatten
  C*P*P = 192 ç»´å‘é‡

ç¤ºä¾‹ Patch 0 (å·¦ä¸Šè§’):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ R:8Ã—8   â”‚ â”€â”
â”‚ G:8Ã—8   â”‚  â”œâ”€ å±•å¹³æˆ [64ä¸ªR, 64ä¸ªG, 64ä¸ªB]
â”‚ B:8Ã—8   â”‚ â”€â”˜    = 192 ç»´å‘é‡
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ•°æ®å¸ƒå±€ï¼š**
```
å±•å¹³å:
(2, 16, 192)
 â”‚  â”‚   â””â”€ æ¯ä¸ª patch æ˜¯ 192 ç»´å‘é‡
 â”‚  â””â”€â”€â”€â”€â”€ 16 ä¸ª patches (4Ã—4 grid)
 â””â”€â”€â”€â”€â”€â”€â”€â”€ 2 ä¸ªæ ·æœ¬

Patch åºåˆ—:
[Patch_0, Patch_1, ..., Patch_15]
   192ç»´   192ç»´        192ç»´
```

#### Step 1.4: çº¿æ€§æŠ•å½±åˆ°åµŒå…¥ç©ºé—´

```
è¾“å…¥: (N, num_patches, patch_dim) = (2, 16, 192)
           â†“ Linear(192, 128)
è¾“å‡º: (N, num_patches, embed_dim) = (2, 16, 128)
```

**æŠ•å½±çŸ©é˜µï¼š**
```python
self.proj = nn.Linear(192, 128)

æƒé‡çŸ©é˜µ: (192, 128)
åç½®å‘é‡: (128,)

æ¯ä¸ª patch:
  [192ç»´] Ã— [192Ã—128] + [128] = [128ç»´]
```

**å«ä¹‰ï¼š**
```
åŸå§‹ patch å‘é‡ (192ç»´):
  ç›´æ¥çš„åƒç´ å€¼æ‹¼æ¥
    â†“ å­¦ä¹ æŠ•å½±
åµŒå…¥å‘é‡ (128ç»´):
  é«˜å±‚è¯­ä¹‰è¡¨ç¤º
  åŒ…å« patch çš„è¯­ä¹‰ä¿¡æ¯
```

### Patch Embedding å®Œæ•´ç¤ºä¾‹

```
å…·ä½“æ•°å€¼ç¤ºä¾‹ (Batch 0, Patch 0):

åŸå§‹ patch (3, 8, 8):
  Ré€šé“: [[0.2, 0.3, ...],   â† 8Ã—8 çŸ©é˜µ
          [0.5, 0.1, ...],
          ...]
  Gé€šé“: [...]
  Bé€šé“: [...]

å±•å¹³å (192,):
  [0.2, 0.3, ...(64ä¸ªRå€¼), ...(64ä¸ªGå€¼), ...(64ä¸ªBå€¼)]

æŠ•å½±å (128,):
  [-0.63, 0.03, 0.61, ..., -0.45]  â† å­¦ä¹ åˆ°çš„è¯­ä¹‰å‘é‡
```

### Patch Embedding è¾“å‡º

```
æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: (2, 16, 128)

è§£é‡Š:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Batch 0:                                â”‚
â”‚   Patch 0:  [128ç»´åµŒå…¥å‘é‡]             â”‚
â”‚   Patch 1:  [128ç»´åµŒå…¥å‘é‡]             â”‚
â”‚   ...                                   â”‚
â”‚   Patch 15: [128ç»´åµŒå…¥å‘é‡]             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Batch 1:                                â”‚
â”‚   Patch 0:  [128ç»´åµŒå…¥å‘é‡]             â”‚
â”‚   ...                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¿™ä¸ªåºåˆ—å°†ä¼ å…¥ Transformer!
```

---

## ğŸ¯ Stage 2: Positional Encoding è¯¦è§£

### ç›®æ ‡
ä¸ºæ¯ä¸ª patch æ·»åŠ ä½ç½®ä¿¡æ¯ï¼Œå› ä¸º attention æœ¬èº«ä¸çŸ¥é“é¡ºåºã€‚

### è¾“å…¥è¾“å‡º

```
è¾“å…¥:  (N, num_patches, embed_dim) = (2, 16, 128)
å¤„ç†:  æ·»åŠ ä½ç½®ç¼–ç 
è¾“å‡º:  (N, num_patches, embed_dim) = (2, 16, 128)  â† å½¢çŠ¶ä¸å˜
```

### ä½ç½®ç¼–ç çŸ©é˜µ

é¢„è®¡ç®—çš„å›ºå®šçŸ©é˜µï¼š`PE (1, max_len, embed_dim)`

```
PE çŸ©é˜µ: (1, 50, 128)  â† max_len=50, embed_dim=128
         â†‘   â†‘    â†‘
         â”‚   â”‚    â””â”€ æ¯ä¸ªä½ç½® 128 ç»´ç¼–ç 
         â”‚   â””â”€â”€â”€â”€â”€â”€ æœ€å¤šæ”¯æŒ 50 ä¸ª patches
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ batch ç»´åº¦ä¸º 1ï¼ˆå¹¿æ’­ç”¨ï¼‰
```

### ä½ç½®ç¼–ç å…¬å¼

```
PE(pos, 2i)   = sin(pos / 10000^(2i/d))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d))

å…¶ä¸­:
  pos: ä½ç½®ç´¢å¼• (0, 1, 2, ..., 15)
  i:   ç»´åº¦å¯¹ç´¢å¼• (0, 1, 2, ..., 63)  â† d/2 = 128/2
  d:   åµŒå…¥ç»´åº¦ (128)
```

### è®¡ç®—è¿‡ç¨‹

#### Step 2.1: åˆ›å»ºä½ç½®ç´¢å¼•

```python
position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
# å½¢çŠ¶: (50, 1)

position = [[0.],
            [1.],
            [2.],
            ...
            [49.]]
```

#### Step 2.2: è®¡ç®—é¢‘ç‡é¡¹

```python
div_term = torch.exp(
    torch.arange(0, embed_dim, 2, dtype=torch.float) *
    -(math.log(10000.0) / embed_dim)
)
# å½¢çŠ¶: (64,)  â† embed_dim/2

div_term è®¡ç®—è¿‡ç¨‹:
1) torch.arange(0, 128, 2) = [0, 2, 4, ..., 126]  # å¶æ•°ç´¢å¼•
2) -(math.log(10000) / 128) = -0.0718
3) [0, 2, 4, ...] * -0.0718 = [0, -0.144, -0.287, ...]
4) exp([0, -0.144, -0.287, ...]) = [1.0, 0.866, 0.750, ...]

ç»“æœ: [1.0, 0.866, 0.750, ..., å¾ˆå°çš„æ•°]
      â†‘                          â†‘
    é«˜é¢‘                        ä½é¢‘
```

#### Step 2.3: å¹¿æ’­è®¡ç®—

```python
# å¹¿æ’­: (50, 1) Ã— (64,) â†’ (50, 64)
position * div_term

ç»“æœçŸ©é˜µ (50, 64):
  pos=0: [0.0,   0.0,   0.0,   ...]  â† ä½ç½® 0
  pos=1: [1.0,   0.87,  0.75,  ...]  â† ä½ç½® 1
  pos=2: [2.0,   1.73,  1.50,  ...]  â† ä½ç½® 2
  ...
  pos=15:[15.0, 13.0, 11.25, ...]  â† ä½ç½® 15
```

#### Step 2.4: åº”ç”¨ sin/cos

```python
# å¶æ•°ç»´åº¦: sin
pe[:, :, 0::2] = torch.sin(position * div_term)

# å¥‡æ•°ç»´åº¦: cos
pe[:, :, 1::2] = torch.cos(position * div_term)

æœ€ç»ˆ PE çŸ©é˜µ (1, 50, 128):
  pos=0, dim=0:   sin(0Ã—1.0)    = 0.0
  pos=0, dim=1:   cos(0Ã—1.0)    = 1.0
  pos=0, dim=2:   sin(0Ã—0.87)   = 0.0
  pos=0, dim=3:   cos(0Ã—0.87)   = 1.0
  ...

  pos=1, dim=0:   sin(1Ã—1.0)    = 0.841
  pos=1, dim=1:   cos(1Ã—1.0)    = 0.540
  pos=1, dim=2:   sin(1Ã—0.87)   = 0.764
  pos=1, dim=3:   cos(1Ã—0.87)   = 0.645
  ...
```

### ä½ç½®ç¼–ç å¯è§†åŒ–

```
PE çŸ©é˜µç»“æ„ (åªæ˜¾ç¤ºå‰ 4 ä¸ªä½ç½®, 8 ç»´):

Pos  Dim0   Dim1   Dim2   Dim3   Dim4   Dim5   Dim6   Dim7
     sin    cos    sin    cos    sin    cos    sin    cos
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€
 0   â”‚ 0.00 â”‚ 1.00 â”‚ 0.00 â”‚ 1.00 â”‚ 0.00 â”‚ 1.00 â”‚ 0.00 â”‚ 1.00
 1   â”‚ 0.84 â”‚ 0.54 â”‚ 0.76 â”‚ 0.65 â”‚ 0.68 â”‚ 0.73 â”‚ 0.60 â”‚ 0.80
 2   â”‚ 0.91 â”‚-0.42 â”‚ 0.99 â”‚-0.16 â”‚ 0.95 â”‚-0.30 â”‚ 0.86 â”‚-0.51
 3   â”‚ 0.14 â”‚-0.99 â”‚ 0.56 â”‚-0.83 â”‚ 0.80 â”‚-0.60 â”‚ 0.96 â”‚-0.27
...
15   â”‚-0.65 â”‚ 0.76 â”‚-0.98 â”‚-0.19 â”‚-0.28 â”‚ 0.96 â”‚ 0.40 â”‚ 0.92

æ¯è¡Œæ˜¯ä¸€ä¸ªä½ç½®çš„ç¼–ç ï¼ˆå”¯ä¸€çš„ï¼‰
æ¯å¯¹åˆ—(sin,cos)ä½¿ç”¨ç›¸åŒé¢‘ç‡ï¼Œä¸åŒç›¸ä½
```

### æ·»åŠ ä½ç½®ç¼–ç 

```python
# é€‰æ‹©å‰ 16 ä¸ªä½ç½®çš„ç¼–ç 
pe_for_input = self.pe[:, :16, :]  # (1, 16, 128)

# å¹¿æ’­ç›¸åŠ 
output = x + pe_for_input
# (2, 16, 128) + (1, 16, 128) â†’ (2, 16, 128)
```

**è¯¦ç»†è¿‡ç¨‹ï¼š**
```
è¾“å…¥ x (2, 16, 128):
  Batch 0, Patch 0: [eâ‚€â‚€, eâ‚€â‚, ..., eâ‚€â‚â‚‚â‚‡]  â† å†…å®¹åµŒå…¥
  Batch 0, Patch 1: [eâ‚â‚€, eâ‚â‚, ..., eâ‚â‚â‚‚â‚‡]
  ...

ä½ç½®ç¼–ç  pe (1, 16, 128):
  Patch 0: [pâ‚€â‚€, pâ‚€â‚, ..., pâ‚€â‚â‚‚â‚‡]  â† ä½ç½® 0 çš„ç¼–ç 
  Patch 1: [pâ‚â‚€, pâ‚â‚, ..., pâ‚â‚â‚‚â‚‡]  â† ä½ç½® 1 çš„ç¼–ç 
  ...

ç›¸åŠ å (2, 16, 128):
  Batch 0, Patch 0: [eâ‚€â‚€+pâ‚€â‚€, eâ‚€â‚+pâ‚€â‚, ..., eâ‚€â‚â‚‚â‚‡+pâ‚€â‚â‚‚â‚‡]
  Batch 0, Patch 1: [eâ‚â‚€+pâ‚â‚€, eâ‚â‚+pâ‚â‚, ..., eâ‚â‚â‚‚â‚‡+pâ‚â‚â‚‚â‚‡]
  ...
  Batch 1: åŒæ ·çš„ä½ç½®ç¼–ç !  â† ä½ç½®ç¼–ç å¯¹æ‰€æœ‰æ ·æœ¬ç›¸åŒ
```

### Dropout

```python
output = self.dropout(output)

è®­ç»ƒæ—¶ (dropout=0.1):
  éšæœºå°† 10% çš„å…ƒç´ ç½®é›¶
  å…¶ä½™å…ƒç´  Ã— (1/0.9) ç¼©æ”¾

æ¨ç†æ—¶:
  ç›´æ¥ä¼ é€’ï¼Œä¸åšä»»ä½•æ“ä½œ
```

### Positional Encoding æ€»ç»“

```
è¾“å…¥:  (2, 16, 128)  â† çº¯å†…å®¹åµŒå…¥
  +
ä½ç½®:  (1, 16, 128)  â† å›ºå®šä½ç½®ç¼–ç 
  =
è¾“å‡º:  (2, 16, 128)  â† å†…å®¹ + ä½ç½®ä¿¡æ¯

ç°åœ¨æ¯ä¸ª patch åµŒå…¥æ—¢åŒ…å«:
  1. å†…å®¹ä¿¡æ¯ (è¿™ä¸ª patch çš„è§†è§‰ç‰¹å¾)
  2. ä½ç½®ä¿¡æ¯ (è¿™ä¸ª patch åœ¨å›¾åƒä¸­çš„ä½ç½®)
```

**ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ**
```
Self-Attention æ˜¯ permutation-invariant:
  [Patch_0, Patch_1, Patch_2]
  [Patch_2, Patch_0, Patch_1]  â† æ‰“ä¹±é¡ºåº
  [Patch_1, Patch_2, Patch_0]

  â†’ Attention çš„è¾“å‡ºå®Œå…¨ç›¸åŒ! âœ—

æ·»åŠ ä½ç½®ç¼–ç å:
  æ¯ä¸ªä½ç½®æœ‰å”¯ä¸€çš„ç¼–ç 
  â†’ Attention èƒ½åŒºåˆ†ä¸åŒä½ç½® âœ“
```

---

## ğŸ”„ Stage 3: Transformer Encoder Layer è¯¦è§£

ç°åœ¨æ•°æ®è¿›å…¥ Transformer Encoderï¼Œç»è¿‡ 6 å±‚ç›¸åŒçš„å¤„ç†ã€‚

### å•å±‚ç»“æ„

```
è¾“å…¥: (N, L, D) = (2, 16, 128)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sub-layer 1: Multi-Head Attention   â”‚
â”‚   â”œâ”€ Self-Attention                 â”‚
â”‚   â”œâ”€ Dropout                        â”‚
â”‚   â”œâ”€ Residual Connection            â”‚
â”‚   â””â”€ Layer Normalization            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (2, 16, 128)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sub-layer 2: Feedforward Network    â”‚
â”‚   â”œâ”€ Linear â†’ GELU â†’ Linear         â”‚
â”‚   â”œâ”€ Dropout                        â”‚
â”‚   â”œâ”€ Residual Connection            â”‚
â”‚   â””â”€ Layer Normalization            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (2, 16, 128)
è¾“å‡º: (N, L, D) = (2, 16, 128)
```

### Sub-layer 1: Multi-Head Self-Attention

#### è¾“å…¥
```
src: (N, L, D) = (2, 16, 128)
```

#### Step 3.1.1: è®¡ç®— Q, K, V

```python
Q = self.query(src)  # Linear(128, 128)
K = self.key(src)    # Linear(128, 128)
V = self.value(src)  # Linear(128, 128)

æ¯ä¸ªéƒ½æ˜¯: (2, 16, 128)
```

**è¯¦ç»†è¿‡ç¨‹ï¼š**
```
å¯¹äºæ¯ä¸ª patch å‘é‡ x (128ç»´):
  q = W_Q @ x + b_Q  â† æŸ¥è¯¢å‘é‡
  k = W_K @ x + b_K  â† é”®å‘é‡
  v = W_V @ x + b_V  â† å€¼å‘é‡

æ‰€æœ‰ patch:
  Q = [qâ‚€, qâ‚, ..., qâ‚â‚…]  (16, 128)
  K = [kâ‚€, kâ‚, ..., kâ‚â‚…]  (16, 128)
  V = [vâ‚€, vâ‚, ..., vâ‚â‚…]  (16, 128)
```

#### Step 3.1.2: åˆ†å‰²æˆå¤šå¤´

```python
# num_heads = 4, head_dim = 128/4 = 32

Q = Q.view(N, L, num_heads, head_dim).transpose(1, 2)
# (2, 16, 128) â†’ (2, 16, 4, 32) â†’ (2, 4, 16, 32)

K = K.view(N, L, num_heads, head_dim).transpose(1, 2)
V = V.view(N, L, num_heads, head_dim).transpose(1, 2)

æ‰€æœ‰éƒ½æ˜¯: (N, H, L, D_h) = (2, 4, 16, 32)
           â†‘  â†‘  â†‘   â†‘
           â”‚  â”‚  â”‚   â””â”€ æ¯å¤´ç»´åº¦ = 32
           â”‚  â”‚  â””â”€â”€â”€â”€â”€ åºåˆ—é•¿åº¦ = 16
           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€ å¤´æ•° = 4
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ batch = 2
```

**å¯è§†åŒ–åˆ†å‰²ï¼š**
```
åŸå§‹ Q (2, 16, 128):
  Batch 0:
    Patch 0: [qâ‚€, qâ‚, qâ‚‚, ..., qâ‚â‚‚â‚‡]  â† 128ç»´
    Patch 1: [...]
    ...

åˆ†å‰²æˆ 4 å¤´å (2, 4, 16, 32):
  Batch 0:
    Head 0:
      Patch 0: [qâ‚€, qâ‚, ..., qâ‚ƒâ‚]     â† 32ç»´ (å‰1/4)
      Patch 1: [...]
      ...
    Head 1:
      Patch 0: [qâ‚ƒâ‚‚, qâ‚ƒâ‚ƒ, ..., qâ‚†â‚ƒ]   â† 32ç»´ (ç¬¬2ä¸ª1/4)
      ...
    Head 2:
      Patch 0: [qâ‚†â‚„, ..., qâ‚‰â‚…]        â† 32ç»´ (ç¬¬3ä¸ª1/4)
      ...
    Head 3:
      Patch 0: [qâ‚‰â‚†, ..., qâ‚â‚‚â‚‡]       â† 32ç»´ (æœ€å1/4)
      ...
  Batch 1:
    ...
```

#### Step 3.1.3: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°

```python
scores = Q @ K.transpose(-2, -1) / sqrt(head_dim)
# (2, 4, 16, 32) @ (2, 4, 32, 16) â†’ (2, 4, 16, 16)
```

**è¯¦ç»†è¿‡ç¨‹ï¼š**
```
å¯¹äº Head 0, Batch 0:
  Q: (16, 32)  â† 16ä¸ªqueryï¼Œæ¯ä¸ª32ç»´
  K^T: (32, 16)  â† 16ä¸ªkeyè½¬ç½®

  scores = Q @ K^T / sqrt(32)
         = (16, 32) @ (32, 16) / 5.66
         = (16, 16) / 5.66

scores çŸ©é˜µ (16, 16):
       kâ‚€    kâ‚    kâ‚‚   ...  kâ‚â‚…
  qâ‚€ [sâ‚€â‚€  sâ‚€â‚  sâ‚€â‚‚  ...  sâ‚€â‚â‚…]  â† Patch 0 å¯¹æ‰€æœ‰ patch çš„åˆ†æ•°
  qâ‚ [sâ‚â‚€  sâ‚â‚  sâ‚â‚‚  ...  sâ‚â‚…]   â† Patch 1 å¯¹æ‰€æœ‰ patch çš„åˆ†æ•°
  ...
  qâ‚â‚…[sâ‚â‚…â‚€ sâ‚â‚…â‚ ...     sâ‚â‚…â‚â‚…] â† Patch 15 å¯¹æ‰€æœ‰ patch çš„åˆ†æ•°

s_ij = (q_i Â· k_j) / sqrt(32)  â† ç‚¹ç§¯ç›¸ä¼¼åº¦
```

**çŸ©é˜µå«ä¹‰ï¼š**
```
scores[i, j] è¡¨ç¤º:
  Patch i å¯¹ Patch j çš„"æ³¨æ„åŠ›ç¨‹åº¦"

ç¤ºä¾‹ (å‡è®¾æ•°å€¼):
       P0   P1   P2   P3   P4  ...
  P0 [0.8  0.1  0.0  0.0  0.1 ...]  â† P0 ä¸»è¦å…³æ³¨è‡ªå·±å’ŒP1, P4
  P1 [0.1  0.7  0.2  0.0  0.0 ...]  â† P1 ä¸»è¦å…³æ³¨è‡ªå·±å’ŒP2
  P2 [0.0  0.2  0.6  0.2  0.0 ...]  â† P2 å…³æ³¨è‡ªå·±å’Œé‚»è¿‘çš„
  ...

æ¯è¡Œå’Œ = 1 (softmax å)
```

#### Step 3.1.4: Softmax å½’ä¸€åŒ–

```python
attn_weights = softmax(scores, dim=-1)
# (2, 4, 16, 16)
```

**Softmax è¿‡ç¨‹ï¼š**
```
scores çš„æ¯ä¸€è¡Œ:
  [2.3, 0.5, -0.1, 0.2, ...]  â† åŸå§‹åˆ†æ•°
      â†“ softmax
  [0.71, 0.11, 0.06, 0.08, ...] â† å½’ä¸€åŒ–æƒé‡
   â†‘
   å’Œä¸º 1.0

æ¯ä¸ª patch çš„æ³¨æ„åŠ›åˆ†å¸ƒ:
  Patch 0 â†’ [0.71åˆ°P0, 0.11åˆ°P1, 0.06åˆ°P2, ...]
  Patch 1 â†’ [...]
  ...
```

**å¯è§†åŒ–æ³¨æ„åŠ›çŸ©é˜µï¼š**
```
attn_weights (16, 16) - Head 0, Batch 0:

       Attend to â†’
       P0   P1   P2   P3   P4  ... P15
  P0  [.71  .11  .06  .02  .05 ... .01]  â† æ€»å’Œ=1.0
  P1  [.10  .65  .15  .04  .02 ... .01]  â† æ€»å’Œ=1.0
  P2  [.05  .12  .58  .14  .03 ... .02]
  P3  [.02  .04  .15  .60  .10 ... .01]
  ...
  P15 [.01  .01  .02  .03  .05 ... .80]

å¯¹è§’çº¿é€šå¸¸è¾ƒå¤§ (patch å…³æ³¨è‡ªå·±)
é‚»è¿‘çš„ patches ä¹Ÿæœ‰è¾ƒé«˜æƒé‡
```

#### Step 3.1.5: Dropout (å¯é€‰)

```python
attn_weights = self.attn_drop(attn_weights)

è®­ç»ƒæ—¶:
  éšæœºå°† 10% çš„æ³¨æ„åŠ›æƒé‡ç½®é›¶
  å…¶ä½™æƒé‡é‡æ–°å½’ä¸€åŒ–
```

#### Step 3.1.6: åŠ æƒæ±‚å’Œ

```python
output = attn_weights @ V
# (2, 4, 16, 16) @ (2, 4, 16, 32) â†’ (2, 4, 16, 32)
```

**è¯¦ç»†è¿‡ç¨‹ï¼š**
```
å¯¹äº Patch 0, Head 0:
  attn_weights[0]: [0.71, 0.11, 0.06, ..., 0.01]  â† æƒé‡
  V: [vâ‚€, vâ‚, vâ‚‚, ..., vâ‚â‚…]  â† å€¼å‘é‡ï¼Œæ¯ä¸ª32ç»´

  output[0] = 0.71*vâ‚€ + 0.11*vâ‚ + 0.06*vâ‚‚ + ... + 0.01*vâ‚â‚…
            = åŠ æƒå¹³å‡çš„å€¼å‘é‡ (32ç»´)

å¯¹äºæ‰€æœ‰ patches:
  output: (16, 32)  â† 16ä¸ªè¾“å‡ºå‘é‡ï¼Œæ¯ä¸ª32ç»´
```

**å«ä¹‰ï¼š**
```
æ¯ä¸ª patch çš„è¾“å‡º = æ‰€æœ‰ patch çš„å€¼å‘é‡çš„åŠ æƒå¹³å‡
æƒé‡ = æ³¨æ„åŠ›åˆ†æ•°

ç¤ºä¾‹ï¼š
  Patch 0 å…³æ³¨ï¼š71% è‡ªå·± + 11% P1 + 6% P2 + ...
  â†’ è¾“å‡ºä¸»è¦åŒ…å« P0 çš„ä¿¡æ¯ï¼Œå°‘é‡ P1, P2 çš„ä¿¡æ¯

  Patch 5 å…³æ³¨ï¼š20% P4 + 60% P5 + 15% P6 + ...
  â†’ è¾“å‡ºæ··åˆäº† P4, P5, P6 çš„ä¿¡æ¯ (å±€éƒ¨èšåˆ)
```

#### Step 3.1.7: æ‹¼æ¥å¤šå¤´

```python
# è½¬ç½®å›æ¥: (2, 4, 16, 32) â†’ (2, 16, 4, 32)
output = output.transpose(1, 2)

# æ‹¼æ¥: (2, 16, 4, 32) â†’ (2, 16, 128)
output = output.contiguous().view(N, L, embed_dim)
```

**æ‹¼æ¥è¿‡ç¨‹ï¼š**
```
4 ä¸ªå¤´çš„è¾“å‡º:
  Head 0: (16, 32)  â† ç»´åº¦ 0-31
  Head 1: (16, 32)  â† ç»´åº¦ 32-63
  Head 2: (16, 32)  â† ç»´åº¦ 64-95
  Head 3: (16, 32)  â† ç»´åº¦ 96-127

æ‹¼æ¥ â†’ (16, 128):
  Patch 0: [Head0çš„32ç»´ | Head1çš„32ç»´ | Head2çš„32ç»´ | Head3çš„32ç»´]
         = 128ç»´å‘é‡
```

#### Step 3.1.8: è¾“å‡ºæŠ•å½±

```python
output = self.proj(output)  # Linear(128, 128)
# (2, 16, 128) â†’ (2, 16, 128)
```

**ä½œç”¨ï¼š**
```
å…è®¸ä¸åŒå¤´çš„ä¿¡æ¯äº¤äº’å’Œèåˆ

è¾“å…¥: [Head0 | Head1 | Head2 | Head3]  â† å¤´ä¿¡æ¯éš”ç¦»
      â†“ W_O @ [...]
è¾“å‡º: æ¯ä¸ªç»´åº¦æ··åˆäº†æ‰€æœ‰å¤´çš„ä¿¡æ¯  â† å¤´ä¿¡æ¯èåˆ

è¿™æ˜¯ multi-head attention çš„å…³é”®ï¼
```

#### Step 3.1.9: Residual + Dropout + LayerNorm

```python
# 1. Dropout
output = self.dropout_self(output)

# 2. Residual connection
shortcut = src  # ä¿å­˜çš„åŸå§‹è¾“å…¥
output = output + shortcut

# 3. Layer Normalization
output = self.norm_self(output)
```

**è¯¦ç»†è¿‡ç¨‹ï¼š**
```
åŸå§‹è¾“å…¥ src: (2, 16, 128)
             â†“ ä¿å­˜
           shortcut

Attention è¾“å‡º: (2, 16, 128)
             â†“ dropout (10%)
             â†“ + shortcut (æ®‹å·®è¿æ¥)
åˆå¹¶å: (2, 16, 128)
             â†“ LayerNorm (æ¯ä¸ªå‘é‡ç‹¬ç«‹æ ‡å‡†åŒ–)
è¾“å‡º: (2, 16, 128)
```

**LayerNorm è¿‡ç¨‹ï¼š**
```
å¯¹äºæ¯ä¸ª patch å‘é‡ (128ç»´):
  x = [xâ‚€, xâ‚, ..., xâ‚â‚‚â‚‡]

  1. è®¡ç®—å‡å€¼: Î¼ = mean(x)
  2. è®¡ç®—æ–¹å·®: ÏƒÂ² = var(x)
  3. æ ‡å‡†åŒ–: xÌ‚ = (x - Î¼) / sqrt(ÏƒÂ² + Îµ)
  4. ç¼©æ”¾å¹³ç§»: y = Î³ âŠ™ xÌ‚ + Î²

  è¾“å‡º: y (128ç»´, å‡å€¼â‰ˆ0, æ–¹å·®â‰ˆ1)

æ¯ä¸ª patch ç‹¬ç«‹å¤„ç†!
```

### Multi-Head Attention å®Œæ•´æ•°æ®æµ

```
è¾“å…¥ src: (2, 16, 128)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Q, K, V = Linear(src)               â”‚
â”‚   â†’ (2, 16, 128) Ã— 3                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Split into 4 heads                  â”‚
â”‚   â†’ (2, 4, 16, 32) Ã— 3              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Attention Scores = Q @ K^T / âˆš32    â”‚
â”‚   â†’ (2, 4, 16, 16)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Attention Weights = Softmax(Scores) â”‚
â”‚   â†’ (2, 4, 16, 16)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Output = Weights @ V                â”‚
â”‚   â†’ (2, 4, 16, 32)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Concat Heads                        â”‚
â”‚   â†’ (2, 16, 128)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Output Projection                   â”‚
â”‚   â†’ (2, 16, 128)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dropout + Residual + LayerNorm      â”‚
â”‚   â†’ (2, 16, 128)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è¾“å‡º: (2, 16, 128)
```

### Sub-layer 2: Feedforward Network

#### è¾“å…¥
```
src: (2, 16, 128)  â† æ¥è‡ª attention çš„è¾“å‡º
```

#### Step 3.2.1: ç¬¬ä¸€å±‚çº¿æ€§å˜æ¢ + æ¿€æ´»

```python
# FC1: æ‰©å±•ç»´åº¦
x = self.fc1(src)  # Linear(128, 512)
# (2, 16, 128) â†’ (2, 16, 512)

# GELU æ¿€æ´»
x = self.gelu(x)
# (2, 16, 512) â†’ (2, 16, 512)
```

**è¯¦ç»†è¿‡ç¨‹ï¼š**
```
å¯¹äºæ¯ä¸ª patch å‘é‡ (128ç»´):
  input: [iâ‚€, iâ‚, ..., iâ‚â‚‚â‚‡]
      â†“ Linear(128, 512)
  hidden: [hâ‚€, hâ‚, ..., hâ‚…â‚â‚]  â† 512ç»´
      â†“ GELU æ¿€æ´»
  activated: [aâ‚€, aâ‚, ..., aâ‚…â‚â‚]

GELU (Gaussian Error Linear Unit):
  GELU(x) = x * Î¦(x)
  å…¶ä¸­ Î¦(x) æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„CDF

  æ¯” ReLU æ›´å¹³æ»‘ï¼Œæœ‰å°çš„è´Ÿå€¼
```

**å¯è§†åŒ–ç»´åº¦å˜åŒ–ï¼š**
```
(2, 16, 128)  â† è¾“å…¥
     â†“ FC1
(2, 16, 512)  â† æ‰©å±• 4 å€
     â†“ GELU
(2, 16, 512)  â† éçº¿æ€§

ä¸ºä»€ä¹ˆæ‰©å±•ï¼Ÿ
  åˆ›å»ºæ›´é«˜ç»´çš„è¡¨ç¤ºç©ºé—´
  å¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›
  ç±»ä¼¼äºç“¶é¢ˆæ¶æ„çš„æ‰©å±•
```

#### Step 3.2.2: Dropout

```python
x = self.dropout(x)
# (2, 16, 512) â†’ (2, 16, 512)

è®­ç»ƒæ—¶: éšæœºä¸¢å¼ƒ 10%
æ¨ç†æ—¶: ç›´æ¥ä¼ é€’
```

#### Step 3.2.3: ç¬¬äºŒå±‚çº¿æ€§å˜æ¢

```python
x = self.fc2(x)  # Linear(512, 128)
# (2, 16, 512) â†’ (2, 16, 128)
```

**è¯¦ç»†è¿‡ç¨‹ï¼š**
```
å¯¹äºæ¯ä¸ª patch:
  hidden: [hâ‚€, hâ‚, ..., hâ‚…â‚â‚]  â† 512ç»´
      â†“ Linear(512, 128)
  output: [oâ‚€, oâ‚, ..., oâ‚â‚‚â‚‡]  â† 128ç»´

å‹ç¼©å›åŸå§‹ç»´åº¦
```

#### Step 3.2.4: Residual + Dropout + LayerNorm

```python
# Dropout
x = self.dropout_ffn(x)

# Residual
shortcut = src  # FFN è¾“å…¥
x = x + shortcut

# LayerNorm
x = self.norm_ffn(x)
```

**å®Œæ•´æµç¨‹ï¼š**
```
è¾“å…¥: (2, 16, 128)
    â†“ ä¿å­˜ä¸º shortcut
    â†“ FC1(128â†’512) + GELU
(2, 16, 512)
    â†“ Dropout
    â†“ FC2(512â†’128)
(2, 16, 128)
    â†“ Dropout
    â†“ + shortcut (æ®‹å·®)
(2, 16, 128)
    â†“ LayerNorm
è¾“å‡º: (2, 16, 128)
```

### Feedforward å®Œæ•´æ•°æ®æµ

```
è¾“å…¥ src: (2, 16, 128)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FC1: Linear(128, 512)               â”‚
â”‚   â†’ (2, 16, 512)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GELU Activation                     â”‚
â”‚   â†’ (2, 16, 512)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dropout                             â”‚
â”‚   â†’ (2, 16, 512)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FC2: Linear(512, 128)               â”‚
â”‚   â†’ (2, 16, 128)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dropout + Residual + LayerNorm      â”‚
â”‚   â†’ (2, 16, 128)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è¾“å‡º: (2, 16, 128)
```

### å•ä¸ª Encoder Layer æ€»ç»“

```
Layer Input: (2, 16, 128)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Head Self-Attention        â”‚
â”‚  â€¢ 4 heads, each 32-dim           â”‚
â”‚  â€¢ Attention matrix: (16, 16)     â”‚
â”‚  â€¢ Output: (2, 16, 128)           â”‚
â”‚  â€¢ + Residual + LayerNorm         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (2, 16, 128)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feedforward Network               â”‚
â”‚  â€¢ Expand: 128 â†’ 512              â”‚
â”‚  â€¢ GELU activation                â”‚
â”‚  â€¢ Compress: 512 â†’ 128            â”‚
â”‚  â€¢ + Residual + LayerNorm         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Layer Output: (2, 16, 128)

å½¢çŠ¶å§‹ç»ˆä¿æŒ (2, 16, 128)!
```

### 6 å±‚ Encoder å †å 

```
Input: (2, 16, 128)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encoder Layer 1                 â”‚  â† å­¦ä¹ ä½å±‚ç‰¹å¾
â”‚  â€¢ Self-Attention               â”‚
â”‚  â€¢ Feedforward                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (2, 16, 128)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encoder Layer 2                 â”‚
â”‚  â€¢ Self-Attention               â”‚
â”‚  â€¢ Feedforward                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (2, 16, 128)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encoder Layer 3                 â”‚
â”‚  â€¢ Self-Attention               â”‚
â”‚  â€¢ Feedforward                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (2, 16, 128)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encoder Layer 4                 â”‚
â”‚  â€¢ Self-Attention               â”‚
â”‚  â€¢ Feedforward                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (2, 16, 128)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encoder Layer 5                 â”‚
â”‚  â€¢ Self-Attention               â”‚
â”‚  â€¢ Feedforward                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (2, 16, 128)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encoder Layer 6                 â”‚  â† å­¦ä¹ é«˜å±‚è¯­ä¹‰
â”‚  â€¢ Self-Attention               â”‚
â”‚  â€¢ Feedforward                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Output: (2, 16, 128)

æ¯å±‚éƒ½åœ¨ä¸åŒæŠ½è±¡å±‚æ¬¡ä¸Šå¤„ç†ä¿¡æ¯!
```

**å±‚æ¬¡åŒ–ç‰¹å¾å­¦ä¹ ï¼š**
```
Layer 1-2: å­¦ä¹ å±€éƒ¨æ¨¡å¼
  â€¢ ç›¸é‚» patch ä¹‹é—´çš„å…³ç³»
  â€¢ è¾¹ç¼˜ã€çº¹ç†ç­‰ä½å±‚ç‰¹å¾

Layer 3-4: å­¦ä¹ ä¸­å±‚ç‰¹å¾
  â€¢ æ›´å¤§èŒƒå›´çš„ç©ºé—´å…³ç³»
  â€¢ å½¢çŠ¶ã€éƒ¨ä»¶ç­‰ä¸­å±‚æ¦‚å¿µ

Layer 5-6: å­¦ä¹ é«˜å±‚è¯­ä¹‰
  â€¢ å…¨å±€ä¸Šä¸‹æ–‡
  â€¢ å¯¹è±¡çº§åˆ«çš„è¯­ä¹‰ç‰¹å¾
```

---

## ğŸ¯ Stage 4: Global Average Pooling è¯¦è§£

### ç›®æ ‡
å°†åºåˆ—è¡¨ç¤ºèšåˆæˆå•ä¸ªå›¾åƒçº§ç‰¹å¾å‘é‡ã€‚

### è¾“å…¥è¾“å‡º

```
è¾“å…¥:  (N, num_patches, embed_dim) = (2, 16, 128)
å¤„ç†:  å¯¹ patch ç»´åº¦æ±‚å¹³å‡
è¾“å‡º:  (N, embed_dim) = (2, 128)
```

### å¹³å‡æ± åŒ–è¿‡ç¨‹

```python
x = torch.mean(x, dim=1)
# (2, 16, 128) â†’ (2, 128)
```

**è¯¦ç»†è¿‡ç¨‹ï¼š**
```
è¾“å…¥ (2, 16, 128):
  Batch 0:
    Patch 0:  [pâ‚€â‚€, pâ‚€â‚, ..., pâ‚€â‚â‚‚â‚‡]
    Patch 1:  [pâ‚â‚€, pâ‚â‚, ..., pâ‚â‚â‚‚â‚‡]
    ...
    Patch 15: [pâ‚â‚…â‚€, pâ‚â‚…â‚, ..., pâ‚â‚…â‚â‚‚â‚‡]

å¯¹æ¯ä¸ªç»´åº¦æ±‚å¹³å‡:
  dim 0: (pâ‚€â‚€ + pâ‚â‚€ + ... + pâ‚â‚…â‚€) / 16
  dim 1: (pâ‚€â‚ + pâ‚â‚ + ... + pâ‚â‚…â‚) / 16
  ...
  dim 127: (pâ‚€â‚â‚‚â‚‡ + pâ‚â‚â‚‚â‚‡ + ... + pâ‚â‚…â‚â‚‚â‚‡) / 16

è¾“å‡º Batch 0: [avgâ‚€, avgâ‚, ..., avgâ‚â‚‚â‚‡]  â† 128ç»´
```

**å¯è§†åŒ–ï¼š**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 16 ä¸ª patch å‘é‡ (æ¯ä¸ª 128 ç»´)        â”‚
â”‚                                      â”‚
â”‚ Patch 0:  [0.5, -0.2, 0.8, ...]     â”‚
â”‚ Patch 1:  [0.3,  0.1, 0.6, ...]     â”‚
â”‚ Patch 2:  [-0.1, 0.4, 0.9, ...]     â”‚
â”‚ ...                                  â”‚
â”‚ Patch 15: [0.2, -0.3, 0.7, ...]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ mean(dim=1)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å…¨å±€ç‰¹å¾å‘é‡ (128 ç»´)                 â”‚
â”‚                                      â”‚
â”‚ [0.23, 0.05, 0.72, ...]             â”‚
â”‚  â†‘     â†‘     â†‘                      â”‚
â”‚  æ‰€æœ‰  æ‰€æœ‰  æ‰€æœ‰                     â”‚
â”‚  Pçš„   Pçš„   Pçš„                     â”‚
â”‚  dim0  dim1  dim2                    â”‚
â”‚  å¹³å‡  å¹³å‡  å¹³å‡                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä¸ºä»€ä¹ˆç”¨å¹³å‡æ± åŒ–ï¼Ÿ

**æ›¿ä»£æ–¹æ¡ˆå¯¹æ¯”ï¼š**

```
æ–¹æ¡ˆ 1: CLS Token (BERT é£æ ¼)
  æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„ [CLS] token
  åªç”¨å®ƒçš„è¾“å‡ºåšåˆ†ç±»
  ä¼˜ç‚¹: å¯å­¦ä¹ çš„èšåˆ
  ç¼ºç‚¹: éœ€è¦é¢å¤–çš„ token

æ–¹æ¡ˆ 2: Max Pooling
  å¯¹æ¯ä¸ªç»´åº¦å–æœ€å¤§å€¼
  ä¼˜ç‚¹: æ•æ‰æœ€æ˜¾è‘—çš„ç‰¹å¾
  ç¼ºç‚¹: ä¸¢å¤±å¤§é‡ä¿¡æ¯

æ–¹æ¡ˆ 3: Global Average Pooling (æˆ‘ä»¬ç”¨çš„)
  å¯¹æ‰€æœ‰ patch æ±‚å¹³å‡
  ä¼˜ç‚¹: ç®€å•ã€ç¨³å®šã€åˆ©ç”¨æ‰€æœ‰ä¿¡æ¯
  ç¼ºç‚¹: å¹³æ»‘äº†å±€éƒ¨å³°å€¼
```

**å¹³å‡æ± åŒ–çš„ä¼˜åŠ¿ï¼š**
```
1. ç®€å•é«˜æ•ˆ
   â€¢ æ— éœ€é¢å¤–å‚æ•°
   â€¢ è®¡ç®—ç®€å•

2. åˆ©ç”¨æ‰€æœ‰ä¿¡æ¯
   â€¢ æ¯ä¸ª patch éƒ½æœ‰è´¡çŒ®
   â€¢ ä¸ä¼šä¸¢å¤±ä¿¡æ¯

3. å¹³ç§»ä¸å˜æ€§
   â€¢ å¯¹ patch é¡ºåºä¸æ•æ„Ÿ
   â€¢ é€‚åˆå›¾åƒåˆ†ç±»

4. é˜²æ­¢è¿‡æ‹Ÿåˆ
   â€¢ å¼ºåˆ¶æ¨¡å‹ä½¿ç”¨æ‰€æœ‰ patch
   â€¢ ä¸èƒ½åªä¾èµ–æŸä¸ª patch
```

### è¾“å‡º

```
(2, 128)  â† æ¯å¼ å›¾åƒä¸€ä¸ª 128 ç»´ç‰¹å¾å‘é‡

Batch 0: [0.23, 0.05, 0.72, ..., -0.15]  â† å›¾åƒ 0 çš„å…¨å±€ç‰¹å¾
Batch 1: [-0.10, 0.32, 0.45, ..., 0.28]  â† å›¾åƒ 1 çš„å…¨å±€ç‰¹å¾

è¿™ä¸ªå‘é‡åŒ…å«äº†æ•´å¼ å›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ï¼
```

---

## ğŸ“ Stage 5: Classification Head è¯¦è§£

### ç›®æ ‡
å°†å…¨å±€ç‰¹å¾å‘é‡æ˜ å°„åˆ°ç±»åˆ«åˆ†æ•°ã€‚

### è¾“å…¥è¾“å‡º

```
è¾“å…¥:  (N, embed_dim) = (2, 128)
å¤„ç†:  Linear(128, num_classes)
è¾“å‡º:  (N, num_classes) = (2, 10)
```

### çº¿æ€§åˆ†ç±»å™¨

```python
logits = self.head(x)  # Linear(128, 10)
# (2, 128) â†’ (2, 10)
```

**è¯¦ç»†è¿‡ç¨‹ï¼š**
```
æƒé‡çŸ©é˜µ: W (128, 10)
åç½®å‘é‡: b (10,)

å¯¹äº Batch 0:
  feature: [fâ‚€, fâ‚, ..., fâ‚â‚‚â‚‡]  â† 128ç»´
      â†“ W @ feature + b
  logits: [lâ‚€, lâ‚, ..., lâ‚‰]     â† 10ç»´

æ¯ä¸ª logit å¯¹åº”ä¸€ä¸ªç±»åˆ«çš„"åˆ†æ•°"
```

**æƒé‡çŸ©é˜µå¯è§†åŒ–ï¼š**
```
W (128, 10):
       Class0  Class1  ...  Class9
Dim0  [ wâ‚€â‚€    wâ‚€â‚    ...   wâ‚€â‚‰  ]
Dim1  [ wâ‚â‚€    wâ‚â‚    ...   wâ‚â‚‰  ]
...
Dim127[ wâ‚â‚‚â‚‡â‚€  wâ‚â‚‚â‚‡â‚  ...   wâ‚â‚‚â‚‡â‚‰]

æ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªç±»åˆ«çš„"æ¨¡æ¿"
feature ä¸æ¨¡æ¿çš„åŒ¹é…åº¦ â†’ è¯¥ç±»åˆ«çš„åˆ†æ•°
```

### Logits å«ä¹‰

```
è¾“å‡º (2, 10):
  Batch 0: [2.3, -0.5, 1.8, 0.2, -1.1, 0.9, -0.3, 1.5, 0.7, -0.8]
            â†‘    â†‘     â†‘    â†‘     â†‘     â†‘    â†‘     â†‘    â†‘     â†‘
          Cat  Dog  Bird Deer  Frog Ship Plane Car Truck Horse

è¿™äº›æ˜¯æœªå½’ä¸€åŒ–çš„åˆ†æ•° (logits)
```

### Softmax + é¢„æµ‹

```python
# è®­ç»ƒæ—¶: ç›´æ¥ç”¨ logits è®¡ç®— CrossEntropyLoss
loss = F.cross_entropy(logits, labels)

# æ¨ç†æ—¶: è½¬æ¢ä¸ºæ¦‚ç‡
probs = F.softmax(logits, dim=-1)
# (2, 10)

# é¢„æµ‹ç±»åˆ«
preds = logits.argmax(dim=-1)
# (2,)
```

**Softmax è¿‡ç¨‹ï¼š**
```
Logits Batch 0: [2.3, -0.5, 1.8, 0.2, -1.1, 0.9, -0.3, 1.5, 0.7, -0.8]
         â†“ softmax
Probs:  [0.52, 0.03, 0.31, 0.06, 0.02, 0.13, 0.04, 0.23, 0.10, 0.02]
         â†‘                                                       â†‘
       æœ€é«˜æ¦‚ç‡ (é¢„æµ‹: Cat)                                    æ€»å’Œ=1.0

Prediction: argmax = 0 â†’ Cat
```

### å®Œæ•´åˆ†ç±»æµç¨‹

```
Image Features: (N, 128)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linear(128, 10)                     â”‚
â”‚   W: (128, 10)                      â”‚
â”‚   b: (10,)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Logits: (N, 10)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Softmax (æ¨ç†æ—¶)                     â”‚
â”‚   exp(logits) / sum(exp(logits))   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Probabilities: (N, 10)
    â†“
Prediction: argmax â†’ class index
```

---

## ğŸ¬ å®Œæ•´ ViT æ•°æ®æµæ€»ç»“

### å½¢çŠ¶å˜åŒ–è¿½è¸ª

```
è¾“å…¥å›¾åƒ:               (N, 3, 32, 32)
    â†“ Patch Embedding
Patch åºåˆ—:             (N, 16, 128)
    â†“ + Positional Encoding
ä½ç½®ç¼–ç åºåˆ—:           (N, 16, 128)
    â†“ Transformer Layer 1
    â†“ Transformer Layer 2
    â†“ Transformer Layer 3
    â†“ Transformer Layer 4
    â†“ Transformer Layer 5
    â†“ Transformer Layer 6
ç¼–ç ååºåˆ—:             (N, 16, 128)
    â†“ Global Average Pooling
å…¨å±€ç‰¹å¾:               (N, 128)
    â†“ Classification Head
ç±»åˆ« Logits:            (N, 10)
```

### å…·ä½“æ•°å€¼ç¤ºä¾‹ (Batch 0)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: Cat å›¾åƒ (3, 32, 32)                            â”‚
â”‚   RGB åƒç´ å€¼                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Patches: 16ä¸ª 8Ã—8 çš„å›¾åƒå—                             â”‚
â”‚   Patch 0: å·¦ä¸Šè§’ (çŒ«çš„è€³æœµ)                           â”‚
â”‚   Patch 5: ä¸­é—´ (çŒ«çš„çœ¼ç›)                             â”‚
â”‚   Patch 10: å³ä¾§ (çŒ«çš„èƒ¡é¡»)                            â”‚
â”‚   ...                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ Embedding
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embeddings: 16 Ã— 128ç»´å‘é‡                             â”‚
â”‚   æ¯ä¸ªå‘é‡ç¼–ç ä¸€ä¸ª patch çš„è¯­ä¹‰                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ + Position
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ + ä½ç½®ä¿¡æ¯                                             â”‚
â”‚   ç°åœ¨çŸ¥é“ Patch 0 åœ¨å·¦ä¸Šï¼ŒPatch 15 åœ¨å³ä¸‹             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ Transformer (6 layers)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1-2: å­¦ä¹ å±€éƒ¨ç‰¹å¾                                â”‚
â”‚   è€³æœµ patch å…³æ³¨é™„è¿‘çš„ patches                        â”‚
â”‚   çœ¼ç› patch å…³æ³¨å¦ä¸€åªçœ¼ç›çš„ patch                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 3-4: å­¦ä¹ ä¸­å±‚ç‰¹å¾                                â”‚
â”‚   è„¸éƒ¨ patches ç›¸äº’å…³è”                                â”‚
â”‚   èº«ä½“ patches å½¢æˆæ•´ä½“                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 5-6: å­¦ä¹ é«˜å±‚è¯­ä¹‰                                â”‚
â”‚   æ•´å¼ å›¾çš„å…¨å±€ç†è§£                                     â”‚
â”‚   "è¿™æ˜¯ä¸€åªçŒ«" çš„è¯­ä¹‰ä¿¡æ¯                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ Global Average Pooling
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å…¨å±€ç‰¹å¾: 128ç»´å‘é‡                                    â”‚
â”‚   èšåˆäº†æ‰€æœ‰ patch çš„ä¿¡æ¯                              â”‚
â”‚   è¡¨ç¤ºæ•´å¼ å›¾åƒçš„è¯­ä¹‰                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ Classification
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Logits: [2.3, -0.5, 1.8, ...]                         â”‚
â”‚          Cat   Dog  Bird                               â”‚
â”‚          â†‘                                             â”‚
â”‚          æœ€é«˜åˆ† â†’ é¢„æµ‹ä¸º Cat âœ“                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š å‚æ•°æ•°é‡ç»Ÿè®¡

### å„æ¨¡å—å‚æ•°

```
é…ç½®: embed_dim=128, num_heads=4, num_layers=6, dim_feedforward=512

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Patch Embedding                                        â”‚
â”‚   Linear(192, 128): 192Ã—128 + 128 = 24,704            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Positional Encoding                                    â”‚
â”‚   å›ºå®šçŸ©é˜µï¼Œæ— å¯å­¦ä¹ å‚æ•°: 0                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Transformer Encoder Layer (Ã—6)                         â”‚
â”‚   Multi-Head Attention:                                â”‚
â”‚     Q: 128Ã—128 + 128 = 16,512                         â”‚
â”‚     K: 128Ã—128 + 128 = 16,512                         â”‚
â”‚     V: 128Ã—128 + 128 = 16,512                         â”‚
â”‚     Proj: 128Ã—128 + 128 = 16,512                      â”‚
â”‚     Subtotal: 66,048                                   â”‚
â”‚                                                        â”‚
â”‚   Feedforward:                                         â”‚
â”‚     FC1: 128Ã—512 + 512 = 66,048                       â”‚
â”‚     FC2: 512Ã—128 + 128 = 65,664                       â”‚
â”‚     Subtotal: 131,712                                  â”‚
â”‚                                                        â”‚
â”‚   LayerNorm (Ã—2):                                      â”‚
â”‚     (128 + 128) Ã— 2 = 512                             â”‚
â”‚                                                        â”‚
â”‚   Per Layer Total: 198,272                             â”‚
â”‚   6 Layers Total: 1,189,632                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Classification Head                                    â”‚
â”‚   Linear(128, 10): 128Ã—10 + 10 = 1,290               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL: ~1.22M parameters                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è®¡ç®—å¤æ‚åº¦

```
ä¸»è¦è®¡ç®—: Self-Attention

æ¯å±‚æ¯ä¸ªæ ·æœ¬:
  Attention: O(LÂ² Ã— D) = O(16Â² Ã— 128) = O(32,768)
  Feedforward: O(L Ã— D Ã— D_ff) = O(16 Ã— 128 Ã— 512) = O(1,048,576)

6 å±‚æ€»è®¡:
  Attention: 6 Ã— 32,768 = 196,608
  Feedforward: 6 Ã— 1,048,576 = 6,291,456

æ€»è®¡ç®—é‡: ~6.5M ops per sample

ä¸ CNN ç›¸æ¯”:
  ViT: è®¡ç®—é›†ä¸­åœ¨ FFNï¼Œé€‚åˆå¹¶è¡Œ
  CNN: è®¡ç®—åˆ†æ•£åœ¨å·ç§¯ï¼Œå±€éƒ¨æ€§å¼º
```

---

## ğŸ’¡ å…³é”®æ´å¯Ÿ

### 1. ViT å¦‚ä½•"çœ‹"å›¾åƒ

```
CNN:
  â”Œâ”€â”€â”€â”€â”€â”
  â”‚ 3Ã—3 â”‚ â† å±€éƒ¨æ„Ÿå—é‡
  â””â”€â”€â”€â”€â”€â”˜
  é€å±‚æ‰©å¤§æ„Ÿå—é‡
  Layer 1: 3Ã—3
  Layer 2: 5Ã—5
  Layer 3: 7Ã—7
  ...

ViT:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  å…¨å±€æ³¨æ„åŠ›       â”‚ â† ç¬¬ä¸€å±‚å°±èƒ½çœ‹åˆ°æ•´å¼ å›¾
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  æ‰€æœ‰ patch å¯ä»¥äº’ç›¸å…³æ³¨
  Layer 1: å…¨å±€
  Layer 2: å…¨å±€
  Layer 3: å…¨å±€
  ...

ç»“æœ:
  ViT æ›´é€‚åˆæ•æ‰é•¿è·ç¦»ä¾èµ–
  ä½†éœ€è¦æ›´å¤šæ•°æ®å­¦ä¹ å±€éƒ¨æ€§
```

### 2. ä¸ºä»€ä¹ˆ ViT éœ€è¦å¤§æ•°æ®é›†

```
CNN å†…ç½®å‡è®¾:
  âœ“ å±€éƒ¨æ€§ (å·ç§¯)
  âœ“ å¹³ç§»ä¸å˜æ€§ (æƒé‡å…±äº«)
  âœ“ å±‚æ¬¡ç»“æ„ (æ± åŒ–)
  â†’ å°æ•°æ®é›†ä¹Ÿèƒ½è®­ç»ƒå¥½

ViT éœ€è¦å­¦ä¹ è¿™äº›:
  âœ— æ²¡æœ‰å±€éƒ¨æ€§å‡è®¾
  âœ— æ²¡æœ‰å¹³ç§»ä¸å˜æ€§
  âœ— æ²¡æœ‰å¼ºåˆ¶å±‚æ¬¡ç»“æ„
  â†’ éœ€è¦å¤§é‡æ•°æ®å­¦ä¹ è¿™äº›æ¨¡å¼

å°æ•°æ®é›†æ”¹è¿›:
  1. å¼ºæ•°æ®å¢å¼º (å¿…é¡»!)
  2. é¢„è®­ç»ƒ (ImageNet)
  3. æ­£åˆ™åŒ– (dropout, weight decay)
  4. å‡å°æ¨¡å‹
```

### 3. æ³¨æ„åŠ›çš„ç‰©ç†æ„ä¹‰

```
Attention Matrix (16, 16):
       P0   P1   P2   P3  ...
  P0  [â—    â—‹    Â·    Â·  ...]  â† P0 ä¸»è¦å…³æ³¨è‡ªå·±
  P1  [â—‹    â—    â—‹    Â·  ...]  â† P1 å…³æ³¨è‡ªå·±å’Œé‚»å±…
  P5  [Â·    Â·    â—‹    â—  ...]  â† P5 å…³æ³¨ç›¸å…³åŒºåŸŸ
  ...

â— = é«˜æ³¨æ„åŠ› (0.5-1.0)
â—‹ = ä¸­æ³¨æ„åŠ› (0.1-0.5)
Â· = ä½æ³¨æ„åŠ› (0.0-0.1)

å­¦åˆ°çš„æ¨¡å¼:
  â€¢ å¯¹è§’çº¿å¼º: patch å…³æ³¨è‡ªå·±
  â€¢ å±€éƒ¨å¼º: ç›¸é‚» patch äº’ç›¸å…³æ³¨
  â€¢ è¯­ä¹‰ç›¸å…³: çŒ«çš„çœ¼ç› â†â†’ çŒ«çš„è€³æœµ
```

---

## ğŸ“ æ€»ç»“

Vision Transformer å°†å›¾åƒè§†ä¸ºåºåˆ—å¤„ç†ï¼š

1. **åˆ†å‰²**: å›¾åƒ â†’ Patch åºåˆ—
2. **åµŒå…¥**: Patches â†’ è¯­ä¹‰å‘é‡
3. **ç¼–ç **: æ·»åŠ ä½ç½®ä¿¡æ¯
4. **å˜æ¢**: å¤šå±‚è‡ªæ³¨æ„åŠ›å­¦ä¹ ç‰¹å¾
5. **èšåˆ**: å¹³å‡æ± åŒ–å¾—åˆ°å…¨å±€è¡¨ç¤º
6. **åˆ†ç±»**: çº¿æ€§å±‚æ˜ å°„åˆ°ç±»åˆ«

**å…³é”®ä¼˜åŠ¿**:
- å…¨å±€è§†é‡ (ç¬¬ä¸€å±‚å°±èƒ½çœ‹åˆ°æ•´å¼ å›¾)
- å¯å¹¶è¡Œ (æ‰€æœ‰ patch åŒæ—¶å¤„ç†)
- å¯æ‰©å±• (æ¨¡å‹è¶Šå¤§è¶Šå¥½ï¼Œç»™è¶³æ•°æ®)

**ä¸»è¦æŒ‘æˆ˜**:
- éœ€è¦å¤§é‡æ•°æ®
- è®¡ç®—æˆæœ¬é«˜ (O(LÂ²))
- ç¼ºä¹å½’çº³åç½®

ViT å¼€å¯äº†è§†è§‰ Transformer çš„æ–°æ—¶ä»£ï¼ğŸš€
