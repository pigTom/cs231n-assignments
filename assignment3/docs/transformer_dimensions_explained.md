# Transformer ä¸­çš„ç»´åº¦æœ¯è¯­è¯¦è§£

## ğŸ¯ ç®€çŸ­ç­”æ¡ˆ

åœ¨ **Inline Question 3 çš„ä¸Šä¸‹æ–‡ä¸­**ï¼Œ**hidden dimension å°±æ˜¯ embedding dimension**ã€‚

ä½†åœ¨æ›´å¹¿æ³›çš„ Transformer æ–‡çŒ®ä¸­ï¼Œè¿™ä¸¤ä¸ªæœ¯è¯­æœ‰ç»†å¾®å·®åˆ«ã€‚è®©æˆ‘è¯¦ç»†è§£é‡Šã€‚

---

## ğŸ“ ç»´åº¦æœ¯è¯­å¯¹ç…§è¡¨

### Vision Transformer (ViT) ä¸­çš„ç»´åº¦

```python
model = VisionTransformer(
    img_size=32,
    patch_size=8,
    in_channels=3,
    embed_dim=128,        # â† è¿™å°±æ˜¯ "hidden dimension"
    num_layers=6,
    num_heads=4,
    dim_feedforward=512,  # â† è¿™æ˜¯ FFN çš„ä¸­é—´ç»´åº¦
    num_classes=10,
    dropout=0.1
)
```

| æœ¯è¯­ | å€¼ | è¯´æ˜ | åˆ«å |
|------|-----|------|------|
| **embed_dim** | 128 | Patch åµŒå…¥åçš„ç»´åº¦ | hidden_dim, d_model, model_dim |
| **dim_feedforward** | 512 | FFN ä¸­é—´å±‚ç»´åº¦ | ffn_dim, d_ff |
| **head_dim** | 32 | æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ | embed_dim / num_heads |
| **num_patches** | 16 | Patch æ•°é‡ï¼ˆåºåˆ—é•¿åº¦ï¼‰ | sequence_length, L |

---

## ğŸ” è¯¦ç»†è§£é‡Š

### 1. Embedding Dimension (embed_dim)

```
è¾“å…¥å›¾åƒ: (N, 3, 32, 32)
    â†“ Patch Embedding
Patches: (N, 16, 128)  â† 128 å°±æ˜¯ embed_dim
    â†“
    æ¯ä¸ª patch æ˜¯ 128 ç»´å‘é‡
```

**ä½œç”¨**ï¼š
- Patch åµŒå…¥åçš„ç»´åº¦
- æ•´ä¸ª Transformer ä¸­æ•°æ®æµåŠ¨çš„ä¸»è¦ç»´åº¦
- åœ¨æ¯ä¸€å±‚ä¸­ä¿æŒä¸å˜

**ä»£ç ä¸­çš„ä½“ç°**ï¼š
```python
# Patch Embedding
self.proj = nn.Linear(patch_dim, embed_dim)  # 192 â†’ 128

# ä¹‹åæ‰€æœ‰æ•°æ®éƒ½æ˜¯ (N, num_patches, embed_dim)
x = self.patch_embed(x)         # (N, 16, 128)
x = self.positional_encoding(x) # (N, 16, 128)
x = self.transformer(x)         # (N, 16, 128)
x = torch.mean(x, dim=1)        # (N, 128)
```

### 2. Hidden Dimension - ä¸¤ç§å«ä¹‰

#### å«ä¹‰ 1: Model Dimension (å¸¸è§) = embed_dim

åœ¨å¤§å¤šæ•° Transformer æ–‡çŒ®å’Œä»£ç ä¸­ï¼Œ**hidden dimension å°±æ˜¯æŒ‡ embed_dim**ã€‚

```python
# åŸå§‹ Transformer è®ºæ–‡æœ¯è¯­
d_model = 512        # "model dimension" = embed_dim
d_ff = 2048          # "feedforward dimension"

# PyTorch å®ç°
nn.TransformerEncoder(
    d_model=512,     # â† è¿™å°±æ˜¯ hidden_dim / embed_dim
    nhead=8,
    dim_feedforward=2048
)

# æˆ‘ä»¬çš„ ViT å®ç°
VisionTransformer(
    embed_dim=128,   # â† è¿™å°±æ˜¯ hidden_dim / d_model
    dim_feedforward=512
)
```

#### å«ä¹‰ 2: Feedforward Hidden Dimension (è¾ƒå°‘ç”¨)

æœ‰æ—¶ "hidden dimension" ä¹Ÿå¯èƒ½æŒ‡ **FFN ä¸­é—´å±‚çš„ç»´åº¦**ã€‚

```python
class FeedForward(nn.Module):
    def __init__(self, embed_dim, hidden_dim):
        self.fc1 = nn.Linear(embed_dim, hidden_dim)     # 128 â†’ 512
        self.fc2 = nn.Linear(hidden_dim, embed_dim)     # 512 â†’ 128

# æ•°æ®æµ
(N, L, 128) â†’ fc1 â†’ (N, L, 512) â†’ fc2 â†’ (N, L, 128)
                          â†‘
                   è¿™é‡Œæ˜¯ 512 "hidden"
```

ä½†åœ¨æˆ‘ä»¬çš„ä»£ç ä¸­ï¼Œè¿™ä¸ªç»´åº¦å« `dim_feedforward`ï¼Œæ‰€ä»¥ä¸ä¼šæ··æ·†ã€‚

---

## ğŸ“ åœ¨ Inline Question 3 ä¸­çš„å«ä¹‰

åœ¨é¢˜ç›®çš„ä¸Šä¸‹æ–‡ä¸­ï¼š

```
"Double the hidden dimension"
```

**æ˜ç¡®æŒ‡çš„æ˜¯ embed_dim (d_model)**ï¼Œå› ä¸ºï¼š

1. **é¢˜ç›®è¯´æ˜**ï¼š"Please ignore the computation cost of QKV and output projection"
   - è¿™è¯´æ˜å…³æ³¨çš„æ˜¯ attention æ ¸å¿ƒè®¡ç®—
   - Attention çš„ä¸»è¦ç»´åº¦å°±æ˜¯ embed_dim

2. **è®¡ç®—å¤æ‚åº¦å…¬å¼**ï¼š
   ```
   Self-Attention Cost = O(LÂ² Ã— D)

   å…¶ä¸­ D = embed_dim = hidden_dim
   ```

3. **ä»£ç å¯¹åº”**ï¼š
   ```python
   model = VisionTransformer(embed_dim=128, ...)  # åŸå§‹
   model = VisionTransformer(embed_dim=256, ...)  # "Double hidden dim"
   ```

---

## ğŸ“Š å®Œæ•´ç»´åº¦æµåŠ¨ç¤ºä¾‹

```python
# é…ç½®
img_size = 32
patch_size = 8
embed_dim = 128        # â† hidden_dim / d_model
num_heads = 4
dim_feedforward = 512  # â† FFN hidden_dim (ä¸åŒçš„ hidden!)

# æ•°æ®æµ
Input Image:           (N, 3, 32, 32)
  â†“ Patch Embedding
Patch Embeddings:      (N, 16, 128)      â† embed_dim å‡ºç°
  â†“ Positional Encoding
  â†“ Transformer Encoder Layer 1
    â†“ Multi-Head Attention
      â”œâ”€ Split into heads: (N, 4, 16, 32)  â† head_dim = 128/4
      â”œâ”€ Attention:        (N, 4, 16, 32)
      â””â”€ Concat:           (N, 16, 128)    â† å›åˆ° embed_dim
    â†“ Add & Norm:          (N, 16, 128)    â† embed_dim ä¿æŒ
    â†“ Feedforward
      â”œâ”€ FC1:              (N, 16, 512)    â† dim_feedforward
      â”œâ”€ GELU
      â””â”€ FC2:              (N, 16, 128)    â† å›åˆ° embed_dim
    â†“ Add & Norm:          (N, 16, 128)    â† embed_dim ä¿æŒ
  â†“ Transformer Encoder Layer 2-6
    ...                    (N, 16, 128)    â† embed_dim å§‹ç»ˆä¿æŒ
  â†“ Global Average Pool
Output Features:         (N, 128)         â† embed_dim
  â†“ Classification Head
Logits:                  (N, 10)
```

**å…³é”®è§‚å¯Ÿ**ï¼š
- `embed_dim = 128` è´¯ç©¿æ•´ä¸ªæ¨¡å‹
- åªæœ‰åœ¨ FFN å†…éƒ¨çŸ­æš‚å˜æˆ `dim_feedforward = 512`
- æ³¨æ„åŠ›æœºåˆ¶å§‹ç»ˆå·¥ä½œåœ¨ `embed_dim` ç©ºé—´

---

## ğŸ”‘ æœ¯è¯­ç»Ÿä¸€è¡¨

ä¸åŒæ–‡çŒ®/æ¡†æ¶ä½¿ç”¨ä¸åŒæœ¯è¯­ï¼Œä½†æŒ‡çš„æ˜¯åŒä¸€ä¸ªä¸œè¥¿ï¼š

| è®ºæ–‡/æ¡†æ¶ | ä¸»ç»´åº¦æœ¯è¯­ | FFN ä¸­é—´ç»´åº¦æœ¯è¯­ |
|----------|-----------|----------------|
| **åŸå§‹ Transformer** | d_model | d_ff |
| **BERT** | hidden_size | intermediate_size |
| **GPT** | n_embd | n_inner |
| **ViT è®ºæ–‡** | D | MLP_dim |
| **æˆ‘ä»¬çš„å®ç°** | embed_dim | dim_feedforward |
| **PyTorch** | d_model | dim_feedforward |
| **Inline Q3** | hidden dimension | - |

**éƒ½æ˜¯åœ¨è¯´åŒä¸€ä¸ªä¸œè¥¿ï¼**

---

## ğŸ’¡ å®ç”¨å»ºè®®

### å¦‚ä½•åˆ¤æ–­ "hidden dimension" æŒ‡ä»€ä¹ˆï¼Ÿ

**ä¸Šä¸‹æ–‡çº¿ç´¢**ï¼š

1. **å¦‚æœåœ¨è®¨è®º Attention**ï¼š
   ```
   "attention with hidden dimension D"
   â†’ æŒ‡ embed_dim
   ```

2. **å¦‚æœåœ¨è®¨è®º FFN**ï¼š
   ```
   "feedforward network with hidden dimension H"
   â†’ å¯èƒ½æŒ‡ dim_feedforward
   â†’ ä½†é€šå¸¸ä¼šæ˜ç¡®è¯´ "feedforward hidden dim"
   ```

3. **å¦‚æœåœ¨è®¨è®ºæ•´ä½“æ¨¡å‹**ï¼š
   ```
   "Transformer with hidden dimension 512"
   â†’ æŒ‡ embed_dim / d_model
   ```

4. **ä»£ç ä¸­çš„å‚æ•°å**ï¼š
   ```python
   # æ˜ç¡®çš„å‚æ•°å
   embed_dim=128           # ä¸»ç»´åº¦
   dim_feedforward=512     # FFN ç»´åº¦

   # æ¨¡ç³Šçš„å‚æ•°å
   hidden_dim=128          # é€šå¸¸æŒ‡ embed_dim
   ```

---

## ğŸ“ å›åˆ° Inline Question 3

ç°åœ¨ä½ åº”è¯¥å®Œå…¨ç†è§£äº†ï¼š

```
"(i) Double the hidden dimension"
```

**å«ä¹‰**ï¼šå°† embed_dim åŠ å€ï¼ˆä» 128 â†’ 256ï¼‰

**å½±å“**ï¼š
```python
# åŸå§‹
model = VisionTransformer(embed_dim=128, ...)
Cost = O(LÂ² Ã— 128)

# åŠ å€
model = VisionTransformer(embed_dim=256, ...)
Cost = O(LÂ² Ã— 256) = 2 Ã— O(LÂ² Ã— 128)
```

**ä¸ºä»€ä¹ˆä¸æ˜¯ dim_feedforwardï¼Ÿ**

å› ä¸ºé¢˜ç›®è¯´äº†ï¼š
> Please ignore the computation cost of QKV and output projection.

è¿™è¯´æ˜åªå…³æ³¨ **attention çš„æ ¸å¿ƒè®¡ç®—**ï¼ˆsoftmax å’ŒåŠ æƒæ±‚å’Œï¼‰ï¼Œè€Œä¸æ˜¯ linear layersã€‚

Attention çš„æ ¸å¿ƒè®¡ç®—åªæ¶‰åŠ `embed_dim`ï¼Œä¸æ¶‰åŠ `dim_feedforward`ï¼

---

## ğŸ¯ æ€»ç»“

| é—®é¢˜ | ç­”æ¡ˆ |
|------|------|
| hidden_dim æ˜¯ embed_dim å—ï¼Ÿ | **æ˜¯çš„**ï¼ˆåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼‰|
| åœ¨ Inline Q3 ä¸­ï¼Ÿ | **æ˜¯çš„**ï¼ŒæŒ‡ embed_dim |
| æœ‰å…¶ä»–å«ä¹‰å—ï¼Ÿ | æœ‰æ—¶æŒ‡ dim_feedforwardï¼Œä½†å¾ˆå°‘è§ |
| å¦‚ä½•ç¡®å®šï¼Ÿ | çœ‹ä¸Šä¸‹æ–‡å’Œå‚æ•°å |

**è®°ä½**ï¼šå½“ä½ çœ‹åˆ° "hidden dimension" æ—¶ï¼Œ**é»˜è®¤ç†è§£ä¸º embed_dim / d_model**ï¼Œé™¤éä¸Šä¸‹æ–‡æ˜ç¡®æŒ‡å‡ºæ˜¯ FFN çš„ä¸­é—´ç»´åº¦ã€‚

---

## ğŸ“š å‚è€ƒ

```python
# æŸ¥çœ‹æˆ‘ä»¬çš„å®ç°
VisionTransformer(
    embed_dim=128,        # â† è¿™æ˜¯ "hidden dimension"
    num_heads=4,          # â†’ head_dim = 128/4 = 32
    dim_feedforward=512,  # â† FFN å†…éƒ¨ç»´åº¦
)

# PyTorch å®˜æ–¹
nn.TransformerEncoder(
    d_model=512,          # â† è¿™æ˜¯ "hidden dimension"
    nhead=8,
    dim_feedforward=2048
)
```

å¸Œæœ›è¿™è§£é‡Šæ¸…æ¥šäº†ï¼ğŸ“
