# Transformer å¿«é€Ÿå‚è€ƒ

## ğŸ¯ æ ¸å¿ƒæ¨¡å¼ï¼ˆå¿…èƒŒï¼‰

```python
# æ¯ä¸ª Transformer å­å±‚çš„æ ‡å‡†æ¨¡å¼
shortcut = x           # 1. ä¿å­˜è¾“å…¥
x = SubLayer(x)        # 2. åº”ç”¨å˜æ¢ï¼ˆAttention æˆ– FFNï¼‰
x = Dropout(x)         # 3. æ­£åˆ™åŒ–
x = x + shortcut       # 4. æ®‹å·®è¿æ¥ï¼ˆæ¢¯åº¦é«˜é€Ÿå…¬è·¯ï¼‰
x = LayerNorm(x)       # 5. æ ‡å‡†åŒ–ï¼ˆç¨³å®šè®­ç»ƒï¼‰
```

## ğŸ“Š æ ¸å¿ƒä½œç”¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Residual     â”‚ å…è®¸æ¢¯åº¦ç›´æ¥ä¼ æ’­ï¼Œè®­ç»ƒæ·±å±‚ç½‘ç»œ          â”‚
â”‚ (æ®‹å·®è¿æ¥)   â”‚ âˆ‚Loss/âˆ‚x = ... Ã— (âˆ‚F/âˆ‚x + 1) â† "+1"  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LayerNorm    â”‚ æ ‡å‡†åŒ–è¾“å‡ºåˆ†å¸ƒï¼Œç¨³å®šè®­ç»ƒ                â”‚
â”‚ (å±‚æ ‡å‡†åŒ–)   â”‚ å‡å€¼=0, æ–¹å·®=1                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dropout      â”‚ éšæœºä¸¢å¼ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ                    â”‚
â”‚              â”‚ è®­ç»ƒæ—¶: 10% ç½®é›¶ï¼Œæ¨ç†æ—¶: æ— æ“ä½œ        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—ï¸ Transformer æ¶æ„

### Encoder Layer (2 ä¸ª block)

```
Input (N, S, D)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Self-Attention      â”‚  Q=K=V=src, åŒå‘
â”‚ + Residual + Norm   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feedforward         â”‚  FC â†’ GELU â†’ FC
â”‚ + Residual + Norm   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
Output (N, S, D)
```

### Decoder Layer (3 ä¸ª block)

```
Input (N, T, D)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Self-Attention      â”‚  Q=K=V=tgt, å•å‘ (causal mask)
â”‚ + Residual + Norm   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cross-Attention     â”‚  Q=tgt, K=V=memory
â”‚ + Residual + Norm   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feedforward         â”‚  FC â†’ GELU â†’ FC
â”‚ + Residual + Norm   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
Output (N, T, D)
```

## ğŸ”‘ å…³é”®åŒºåˆ«

### Self-Attention vs Cross-Attention

```python
# Self-Attention: åºåˆ—å…³æ³¨è‡ªå·±
out = self.self_attn(query=tgt, key=tgt, value=tgt, mask=tgt_mask)
#                         â†‘        â†‘      â†‘       â†‘
#                         éƒ½æ¥è‡ª tgt      æœ‰ mask (decoder)

# Cross-Attention: decoder å…³æ³¨ encoder
out = self.cross_attn(query=tgt, key=memory, value=memory)
#                          â†‘          â†‘           â†‘
#                      æ¥è‡ª tgt    æ¥è‡ª encoder   æ—  mask
```

### Encoder vs Decoder

| ç‰¹æ€§ | Encoder | Decoder |
|------|---------|---------|
| **Block æ•°é‡** | 2 (Attn + FFN) | 3 (Self + Cross + FFN) |
| **Attention ç±»å‹** | åŒå‘ Self-Attention | å•å‘ Self + Cross |
| **Mask** | å¯é€‰ï¼ˆpadding maskï¼‰ | å¿…éœ€ï¼ˆcausal maskï¼‰ |
| **ç”¨é€”** | ç¼–ç è¾“å…¥åºåˆ— | ç”Ÿæˆè¾“å‡ºåºåˆ— |

## ğŸ“ æ•°å­¦å…¬å¼

### Multi-Head Attention

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V

MultiHead(Q, K, V) = Concat(headâ‚, ..., headâ‚•) W^O
  where headáµ¢ = Attention(QWáµ¢^Q, KWáµ¢^K, VWáµ¢^V)
```

### Layer Normalization

```
LayerNorm(x) = Î³ âŠ™ (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²

å…¶ä¸­:
  Î¼ = mean(x)      # æ²¿ç‰¹å¾ç»´åº¦
  ÏƒÂ² = var(x)      # æ²¿ç‰¹å¾ç»´åº¦
  Î³, Î² æ˜¯å¯å­¦ä¹ å‚æ•°
```

### Positional Encoding

```
PE(pos, 2i)   = sin(pos / 10000^(2i/d))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d))
```

## ğŸ¨ Vision Transformer (ViT)

```
Image (N, C, H, W)
    â†“ Patch Embedding
Patches (N, num_patches, D)
    â†“ + Positional Encoding
    â†“ Transformer Encoder (å¤šå±‚)
Features (N, num_patches, D)
    â†“ Global Average Pooling
    â†“ Classification Head
Logits (N, num_classes)
```

## ğŸ–¼ï¸ Image Captioning Transformer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Image     â”‚              â”‚   Caption    â”‚
â”‚  (N, D)      â”‚              â”‚   (N, T)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                             â”‚
       â†“ Visual Projection           â†“ Embedding + PE
    Memory                          Target
   (N, 1, W)                      (N, T, W)
       â”‚                             â”‚
       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚         â†“
       â”‚    Transformer Decoder
       â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚    â”‚ Self-Attention  â”‚ â† causal mask
       â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚         â†“
       â””â”€â”€â”€â”€â–ºCross-Attention   â† å…³æ³¨å›¾åƒ
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
            Feedforward
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
            Output Layer
                 â†“
            Scores (N, T, V)
```

## âš™ï¸ è¶…å‚æ•°å»ºè®®

### è®­ç»ƒ

```python
# è¿‡æ‹Ÿåˆæµ‹è¯•
optimizer = torch.optim.Adam(model.parameters(),
                             lr=5e-3,           # è¾ƒå¤§å­¦ä¹ ç‡
                             weight_decay=0.0)  # æ— æ­£åˆ™åŒ–
epochs = 150-200

# æ­£å¸¸è®­ç»ƒ
optimizer = torch.optim.Adam(model.parameters(),
                             lr=1e-4 to 1e-3,   # é€‚ä¸­å­¦ä¹ ç‡
                             weight_decay=1e-4) # è½»å¾®æ­£åˆ™åŒ–
use_lr_scheduler = True  # å­¦ä¹ ç‡è¡°å‡
```

### æ¨¡å‹é…ç½®

```python
# å°æ¨¡å‹ (å¿«é€Ÿå®éªŒ)
embed_dim = 128
num_heads = 4
num_layers = 4
dim_feedforward = 512

# ä¸­å‹æ¨¡å‹ (è®ºæ–‡å¤ç°)
embed_dim = 512
num_heads = 8
num_layers = 6
dim_feedforward = 2048

# å¤§æ¨¡å‹ (SOTA)
embed_dim = 768 or 1024
num_heads = 12 or 16
num_layers = 12 or 24
dim_feedforward = 3072 or 4096
```

## ğŸ› å¸¸è§é”™è¯¯

### âŒ é”™è¯¯ 1: Dropout ä½ç½®é”™è¯¯
```python
# é”™è¯¯
x = x + self.attn(x)
x = self.dropout(x)  # âœ— ç ´åæ®‹å·®è·¯å¾„

# æ­£ç¡®
x = self.dropout(self.attn(x))
x = x + shortcut     # âœ“
```

### âŒ é”™è¯¯ 2: å¿˜è®° causal mask
```python
# Decoder self-attention å¿…é¡»æœ‰ mask
tgt_mask = torch.tril(torch.ones(T, T))  # ä¸‹ä¸‰è§’
out = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)
```

### âŒ é”™è¯¯ 3: å½¢çŠ¶ä¸åŒ¹é…
```python
# å›¾åƒç‰¹å¾éœ€è¦æ·»åŠ åºåˆ—ç»´åº¦
features: (N, D) â†’ (N, 1, W)  # unsqueeze(1)

# Captions éœ€è¦ embedding
captions: (N, T) â†’ (N, T, W)  # embedding
```

### âŒ é”™è¯¯ 4: å­¦ä¹ ç‡å¤ªå°
```python
# è¿‡æ‹Ÿåˆæ—¶
lr = 1e-3  # âœ— å¤ªå°
lr = 5e-3  # âœ“ åˆé€‚

# æ­£å¸¸è®­ç»ƒæ—¶
lr = 1e-5  # âœ— å¤ªå°
lr = 1e-4  # âœ“ åˆé€‚
```

## ğŸ“ Shape Cheatsheet

```python
# Multi-Head Attention
Input:  (N, S, D)
Q, K, V: (N, S, D) â†’ split â†’ (N, H, S, D/H)
Scores: (N, H, S, S)
Output: (N, S, D)

# Transformer Encoder
Input:  (N, S, D)
Output: (N, S, D)  # å½¢çŠ¶ä¸å˜

# Transformer Decoder
tgt:    (N, T, D)
memory: (N, S, D)
Output: (N, T, D)  # å½¢çŠ¶ä¸ tgt ç›¸åŒ

# Vision Transformer
Input:  (N, C, H, W)
Patches: (N, (H/P)*(W/P), D)
Output: (N, num_classes)

# Image Captioning
features: (N, D) â†’ (N, 1, W)
captions: (N, T) â†’ (N, T, W)
scores:   (N, T, V)
```

## ğŸ’¡ è°ƒè¯•æŠ€å·§

```python
# 1. æ£€æŸ¥å½¢çŠ¶
print(f"Input shape: {x.shape}")
print(f"Output shape: {out.shape}")

# 2. æ£€æŸ¥æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_norm = {param.grad.norm():.4f}")

# 3. æ£€æŸ¥ attention weights
with torch.no_grad():
    attn_weights = torch.softmax(scores, dim=-1)
    print(f"Max attention: {attn_weights.max():.4f}")
    print(f"Min attention: {attn_weights.min():.4f}")

# 4. è¿‡æ‹Ÿåˆä¸€ä¸ª batch
# å¦‚æœæ— æ³•è¿‡æ‹Ÿåˆï¼Œè¯´æ˜å®ç°æœ‰é—®é¢˜
```

## ğŸ“ æ ¸å¿ƒè¦ç‚¹

1. **æ®‹å·®è¿æ¥** = æ¢¯åº¦é«˜é€Ÿå…¬è·¯ = æ·±å±‚ç½‘ç»œçš„å…³é”®
2. **LayerNorm** = ç¨³å®šè®­ç»ƒ = æ›´å¤§å­¦ä¹ ç‡
3. **Multi-Head** = å¤šè§†è§’ = æ›´ä¸°å¯Œçš„è¡¨ç¤º
4. **Position Encoding** = ä½ç½®ä¿¡æ¯ = åºåˆ—å»ºæ¨¡çš„åŸºç¡€
5. **Cross-Attention** = encoder-decoder è¿æ¥ = seq2seq çš„æ ¸å¿ƒ

---

ğŸ“– **è¯¦ç»†æ–‡æ¡£**: `transformer_residual_pattern.md`
ğŸ”¬ **å®ç°ä»£ç **: `cs231n/transformer_layers.py`
ğŸ§ª **æµ‹è¯•è„šæœ¬**: `test_vit_overfit.py`
