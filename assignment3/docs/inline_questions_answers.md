# Transformer Captioning - Inline Questions ç­”æ¡ˆ

## Inline Question 2: ViT åœ¨å°æ•°æ®é›†ä¸Šçš„æ€§èƒ½

### é—®é¢˜
Despite their recent success in large-scale image recognition tasks, ViTs often lag behind traditional CNNs when trained on smaller datasets. What underlying factor contribute to this performance gap? What techniques can be used to improve the performance of ViTs on small datasets?

### ç­”æ¡ˆ

#### æ€§èƒ½å·®è·çš„æ ¹æœ¬åŸå› 

**1. ç¼ºä¹å½’çº³åç½®ï¼ˆInductive Biasï¼‰**

Vision Transformers lack the strong inductive biases inherent in CNNs, such as locality and translation equivariance. CNNs encode these priors directly into their architecture through local receptive fields and weight sharing across spatial locations. Without sufficient training data, ViTs struggle to learn these fundamental image properties from scratch, requiring substantially more examples to discover patterns that CNNs assume by design.

**å…³é”®ç‚¹è¯¦è§£ï¼š**

```
CNN çš„å†…ç½®å‡è®¾:
â”œâ”€ å±€éƒ¨æ€§ï¼ˆLocalityï¼‰
â”‚  â””â”€ å·ç§¯æ ¸åªçœ‹å±€éƒ¨åŒºåŸŸ â†’ å¤©ç„¶å­¦ä¹ å±€éƒ¨ç‰¹å¾
â”œâ”€ å¹³ç§»ä¸å˜æ€§ï¼ˆTranslation Equivarianceï¼‰
â”‚  â””â”€ æƒé‡å…±äº« â†’ åŒæ ·çš„ç‰¹å¾æ£€æµ‹å™¨ç”¨äºæ•´ä¸ªå›¾åƒ
â””â”€ å±‚æ¬¡ç»“æ„ï¼ˆHierarchicalï¼‰
   â””â”€ é€å±‚å¢å¤§æ„Ÿå—é‡ â†’ ä»å±€éƒ¨åˆ°å…¨å±€

ViT çš„æƒ…å†µ:
â”œâ”€ å…¨å±€æ³¨æ„åŠ›ï¼ˆGlobal Attentionï¼‰
â”‚  â””â”€ ç¬¬ä¸€å±‚å°±èƒ½çœ‹åˆ°æ‰€æœ‰ patch â†’ éœ€è¦æ•°æ®å­¦ä¹ ä½•æ—¶å…³æ³¨å±€éƒ¨
â”œâ”€ æ— æƒé‡å…±äº«
â”‚  â””â”€ æ¯ä¸ªä½ç½®ç‹¬ç«‹å­¦ä¹  â†’ éœ€è¦æ›´å¤šæ•°æ®å­¦ä¹ å¹³ç§»ä¸å˜æ€§
â””â”€ å‡åŒ€å¤„ç†
   â””â”€ æ‰€æœ‰å±‚éƒ½æ˜¯ç›¸åŒç»“æ„ â†’ éœ€è¦æ•°æ®å­¦ä¹ å±‚æ¬¡ç‰¹å¾
```

**2. æ›´é«˜çš„æ¨¡å‹å®¹é‡ä¸è‡ªç”±åº¦**

```
å‚æ•°ä½¿ç”¨æ•ˆç‡:
  CNN:  å¼ºçº¦æŸ â†’ å‚æ•°å°‘ä½†åˆ©ç”¨ç‡é«˜
  ViT:  å¼±çº¦æŸ â†’ å‚æ•°å¤šä½†éœ€è¦å¤§é‡æ•°æ®æ¥æœ‰æ•ˆåˆ©ç”¨

å°æ•°æ®é›†åœºæ™¯:
  CNN:  âœ“ å½’çº³åç½®æŒ‡å¯¼å­¦ä¹ 
  ViT:  âœ— è¿‡æ‹Ÿåˆï¼Œæ— æ³•æ³›åŒ–
```

#### æ”¹è¿›æŠ€æœ¯

**1. æ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰** - æœ€é‡è¦ï¼

```python
# å¼ºæ•°æ®å¢å¼ºå¯¹ ViT è‡³å…³é‡è¦
transforms.Compose([
    transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),
    transforms.RandomRotation(15),
    # é«˜çº§å¢å¼º
    RandAugment(num_ops=2, magnitude=9),
    # Mixup / Cutmix
    ...
])

æ•ˆæœï¼šåœ¨å°æ•°æ®é›†ä¸Šï¼Œå¼ºæ•°æ®å¢å¼ºå¯ä»¥æå‡ 5-10% å‡†ç¡®ç‡
```

**2. é¢„è®­ç»ƒä¸è¿ç§»å­¦ä¹ ï¼ˆPre-training & Transfer Learningï¼‰**

```
ç­–ç•¥ï¼š
1. åœ¨å¤§æ•°æ®é›†ï¼ˆImageNetï¼‰ä¸Šé¢„è®­ç»ƒ
2. åœ¨ç›®æ ‡å°æ•°æ®é›†ä¸Šå¾®è°ƒ

ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼š
  é¢„è®­ç»ƒå­¦åˆ°äº†é€šç”¨çš„è§†è§‰ç‰¹å¾å’Œå½’çº³åç½®
  å¾®è°ƒåªéœ€è¦é€‚åº”ç‰¹å®šä»»åŠ¡
```

**3. æ­£åˆ™åŒ–æŠ€æœ¯**

```python
# Dropout
model = VisionTransformer(dropout=0.3)  # å°æ•°æ®é›†ç”¨æ›´å¤§çš„ dropout

# Weight Decay
optimizer = torch.optim.AdamW(
    model.parameters(),
    weight_decay=1e-3  # å°æ•°æ®é›†ç”¨æ›´å¼ºçš„æ­£åˆ™åŒ–
)

# Stochastic Depth
# éšæœºä¸¢å¼ƒå±‚ï¼Œç±»ä¼¼ Dropout ä½†ç”¨äºå±‚
```

**4. çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰**

```
Teacher (å¤§ CNN) â†’ Student (å° ViT)
  ç”¨è®­ç»ƒå¥½çš„ CNN æŒ‡å¯¼ ViT å­¦ä¹ 
  DeiT (Data-efficient ViT) ä½¿ç”¨è¿™ä¸ªæŠ€æœ¯
```

**5. æ··åˆæ¶æ„ï¼ˆHybrid Architecturesï¼‰**

```
CNN Stem + ViT Body:
  ä½¿ç”¨ CNN æå–ä½å±‚ç‰¹å¾ï¼ˆå¼•å…¥å½’çº³åç½®ï¼‰
  ä½¿ç”¨ ViT å¤„ç†é«˜å±‚è¯­ä¹‰ï¼ˆå…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼‰

ç¤ºä¾‹ï¼š
  Early Convolutions ViT
  ConViTï¼ˆå·ç§¯ä¸æ³¨æ„åŠ›çš„æ··åˆï¼‰
```

**6. å‡å°æ¨¡å‹å®¹é‡**

```python
# å°æ•°æ®é›†ç”¨å°æ¨¡å‹
model = VisionTransformer(
    embed_dim=128,      # æ›´å°çš„ç»´åº¦
    num_layers=4,       # æ›´å°‘çš„å±‚
    num_heads=4,
    patch_size=8        # æ›´å¤§çš„ patchï¼ˆå‡å°‘åºåˆ—é•¿åº¦ï¼‰
)
```

### æ€»ç»“è¡¨æ ¼

| æŠ€æœ¯ | æ•ˆæœ | å®ç°éš¾åº¦ | æ¨èä¼˜å…ˆçº§ |
|------|------|---------|-----------|
| **å¼ºæ•°æ®å¢å¼º** | +++++ | ç®€å• | â­â­â­â­â­ |
| **é¢„è®­ç»ƒ** | +++++ | ä¸­ç­‰ | â­â­â­â­â­ |
| **æ­£åˆ™åŒ–** | ++++ | ç®€å• | â­â­â­â­ |
| **å‡å°æ¨¡å‹** | +++ | ç®€å• | â­â­â­â­ |
| **çŸ¥è¯†è’¸é¦** | ++++ | å¤æ‚ | â­â­â­ |
| **æ··åˆæ¶æ„** | +++ | ä¸­ç­‰ | â­â­â­ |

---

## Inline Question 3: ViT Self-Attention è®¡ç®—æˆæœ¬åˆ†æ

### é—®é¢˜
How does the computational cost of the self-attention layers in a ViT change if we independently make the following changes? Please ignore the computation cost of QKV and output projection.

(i) Double the hidden dimension.
(ii) Double the height and width of the input image.
(iii) Double the patch size.
(iv) Double the number of layers.

### ç­”æ¡ˆ

#### å‰ç½®çŸ¥è¯†ï¼šSelf-Attention çš„è®¡ç®—å¤æ‚åº¦

Self-attention çš„ä¸»è¦è®¡ç®—æ­¥éª¤ï¼š

```python
# 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼ˆæœ€è€—æ—¶ï¼‰
scores = Q @ K^T  # (N, num_patches, D/H) @ (N, D/H, num_patches)
                  # â†’ (N, num_patches, num_patches)

# 2. Softmax
attn_weights = softmax(scores / âˆš(D/H))  # (N, num_patches, num_patches)

# 3. åŠ æƒæ±‚å’Œ
output = attn_weights @ V  # (N, num_patches, num_patches) @ (N, num_patches, D/H)
                           # â†’ (N, num_patches, D/H)
```

**å…³é”®å˜é‡ï¼š**
- `N` = batch size
- `L` = sequence length (num_patches) = (H/P) Ã— (W/P)
- `D` = hidden dimension
- `H` = number of heads
- `d` = D/H = dimension per head

**è®¡ç®—å¤æ‚åº¦åˆ†æï¼š**

```
Step 1: Q @ K^T
  æ¯ä¸ªå…ƒç´ : O(d) æ¬¡ä¹˜æ³•
  æ€»å…±: L Ã— L ä¸ªå…ƒç´ 
  å¤æ‚åº¦: O(LÂ² Ã— d)

Step 2: Softmax
  å¤æ‚åº¦: O(LÂ²)

Step 3: Attention @ V
  æ¯ä¸ªå…ƒç´ : O(L) æ¬¡ä¹˜æ³•
  æ€»å…±: L Ã— d ä¸ªå…ƒç´ 
  å¤æ‚åº¦: O(LÂ² Ã— d)

æ€»è®¡ç®—å¤æ‚åº¦: O(LÂ² Ã— d) = O(LÂ² Ã— D/H)

å¯¹äºå¤šå¤´æ³¨æ„åŠ›ï¼ˆH ä¸ªå¤´ï¼‰:
  æ€»å¤æ‚åº¦: O(LÂ² Ã— D)
```

**é‡è¦ç»“è®ºï¼š**
```
Self-Attention çš„è®¡ç®—æˆæœ¬ âˆ LÂ² Ã— D

å…¶ä¸­:
  L = num_patches = (Image_Height / Patch_Size) Ã— (Image_Width / Patch_Size)
  D = hidden_dim
```

---

#### (i) Double the hidden dimension

**åˆ†æï¼š**

```
åŸå§‹: O(LÂ² Ã— D)
åŠ å€ç»´åº¦: O(LÂ² Ã— 2D) = 2 Ã— O(LÂ² Ã— D)
```

**ç­”æ¡ˆï¼š**

**Computational cost increases by a factor of 2 (doubles).**

The attention mechanism computes LÂ² attention weights, and each weight requires D operations for the weighted sum over value vectors. Doubling D directly doubles the computational cost since the sequence length L remains unchanged.

**è¯¦ç»†è¯´æ˜ï¼š**

```
å‡è®¾åŸå§‹é…ç½®:
  L = 16 patches (4Ã—4 grid with patch_size=8 for 32Ã—32 image)
  D = 128

è®¡ç®—æ­¥éª¤:
  Attention scores: (16Ã—16) Ã— (128/H) æ¬¡ä¹˜æ³•
  Weighted sum: (16Ã—16) Ã— (128/H) æ¬¡ä¹˜æ³•
  æ€»æˆæœ¬: O(16Â² Ã— 128)

åŠ å€ D åˆ° 256:
  Attention scores: (16Ã—16) Ã— (256/H) æ¬¡ä¹˜æ³•
  Weighted sum: (16Ã—16) Ã— (256/H) æ¬¡ä¹˜æ³•
  æ€»æˆæœ¬: O(16Â² Ã— 256) = 2 Ã— O(16Â² Ã— 128)
```

---

#### (ii) Double the height and width of the input image

**åˆ†æï¼š**

```
åŸå§‹å›¾åƒ: H Ã— W
æ–°å›¾åƒ: 2H Ã— 2W (é¢ç§¯ Ã— 4)

Patch æ•°é‡:
  åŸå§‹: L = (H/P) Ã— (W/P)
  æ–°çš„: L' = (2H/P) Ã— (2W/P) = 4L

è®¡ç®—å¤æ‚åº¦:
  åŸå§‹: O(LÂ² Ã— D)
  æ–°çš„: O((4L)Â² Ã— D) = O(16LÂ² Ã— D) = 16 Ã— O(LÂ² Ã— D)
```

**ç­”æ¡ˆï¼š**

**Computational cost increases by a factor of 16.**

Doubling both image dimensions quadruples the number of patches (from L to 4L), since patches are extracted from a 2D grid. The self-attention complexity scales quadratically with sequence length, so (4L)Â² = 16LÂ², resulting in a 16Ã— increase in computational cost.

**è¯¦ç»†è¯´æ˜ï¼š**

```
ç¤ºä¾‹: 32Ã—32 å›¾åƒ with patch_size=8

åŸå§‹:
  Grid: 4Ã—4 = 16 patches
  Cost: O(16Â² Ã— D) = O(256D)

åŒå€å°ºå¯¸: 64Ã—64 å›¾åƒ
  Grid: 8Ã—8 = 64 patches = 4 Ã— 16
  Cost: O(64Â² Ã— D) = O(4096D) = 16 Ã— O(256D)

å…³é”®æ´å¯Ÿ:
  å›¾åƒç»´åº¦ Ã— 2 â†’ patch æ•°é‡ Ã— 4 â†’ è®¡ç®—æˆæœ¬ Ã— 16
  è¿™æ˜¯ ViT å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒçš„ä¸»è¦ç“¶é¢ˆï¼
```

**å®é™…å½±å“ï¼š**

```
åˆ†è¾¨ç‡    Patches   è®¡ç®—æˆæœ¬å€æ•°
32Ã—32     16        1Ã—
64Ã—64     64        16Ã—      â† åŒå€å°ºå¯¸
128Ã—128   256       256Ã—     â† 4å€å°ºå¯¸
224Ã—224   784       2401Ã—    â† ImageNet æ ‡å‡†å°ºå¯¸

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ ViT åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸Šå¾ˆæ…¢ï¼
```

---

#### (iii) Double the patch size

**åˆ†æï¼š**

```
Patch size Ã— 2 â†’ æ¯ä¸ªç»´åº¦çš„ patch æ•°é‡ / 2

Patch æ•°é‡:
  åŸå§‹: L = (H/P) Ã— (W/P)
  æ–°çš„: L' = (H/2P) Ã— (W/2P) = L/4

è®¡ç®—å¤æ‚åº¦:
  åŸå§‹: O(LÂ² Ã— D)
  æ–°çš„: O((L/4)Â² Ã— D) = O(LÂ²/16 Ã— D) = (1/16) Ã— O(LÂ² Ã— D)
```

**ç­”æ¡ˆï¼š**

**Computational cost decreases by a factor of 16 (becomes 1/16 of original).**

Doubling the patch size reduces the number of patches by a factor of 4 (since patches are 2D), and since self-attention scales quadratically with sequence length, the computational cost reduces by (1/4)Â² = 1/16. This is a common strategy to reduce computational cost for high-resolution images.

**è¯¦ç»†è¯´æ˜ï¼š**

```
ç¤ºä¾‹: 32Ã—32 å›¾åƒ

åŸå§‹ patch_size=4:
  Grid: 8Ã—8 = 64 patches
  Cost: O(64Â² Ã— D) = O(4096D)

åŒå€ patch_size=8:
  Grid: 4Ã—4 = 16 patches = 64/4
  Cost: O(16Â² Ã— D) = O(256D) = (1/16) Ã— O(4096D)

æƒè¡¡ï¼ˆTrade-offï¼‰:
  ä¼˜ç‚¹: è®¡ç®—æˆæœ¬å¤§å¹…é™ä½
  ç¼ºç‚¹: æŸå¤±ç»†ç²’åº¦çš„ç©ºé—´ä¿¡æ¯
```

**å®é™…åº”ç”¨ï¼š**

```
ViT å˜ä½“:
  ViT-B/16: patch_size=16, é€‚ä¸­çš„è®¡ç®—æˆæœ¬
  ViT-B/32: patch_size=32, æ›´å¿«ä½†æ€§èƒ½ç•¥å·®
  ViT-B/8:  patch_size=8,  æ›´æ…¢ä½†æ€§èƒ½æ›´å¥½

ImageNet (224Ã—224):
  Patch 16Ã—16 â†’ 196 patches â†’ å¯æ¥å—
  Patch 8Ã—8   â†’ 784 patches â†’ å¾ˆæ…¢ï¼ˆ16Ã—ï¼‰
  Patch 32Ã—32 â†’ 49 patches  â†’ å¾ˆå¿«ï¼ˆ1/16ï¼‰
```

---

#### (iv) Double the number of layers

**åˆ†æï¼š**

```
æ¯å±‚çš„è®¡ç®—æˆæœ¬: O(LÂ² Ã— D)
å±‚æ•° Ã— 2 â†’ æ€»æˆæœ¬ Ã— 2
```

**ç­”æ¡ˆï¼š**

**Computational cost increases by a factor of 2 (doubles).**

Each transformer layer performs self-attention independently. Doubling the number of layers simply means performing the same O(LÂ² Ã— D) computation twice as many times, resulting in a linear (2Ã—) increase in total computational cost.

**è¯¦ç»†è¯´æ˜ï¼š**

```
åŸå§‹: 6 layers
  æ¯å±‚: O(LÂ² Ã— D)
  æ€»è®¡: 6 Ã— O(LÂ² Ã— D)

åŒå€: 12 layers
  æ¯å±‚: O(LÂ² Ã— D)
  æ€»è®¡: 12 Ã— O(LÂ² Ã— D) = 2 Ã— [6 Ã— O(LÂ² Ã— D)]

å…³é”®ç‚¹:
  å±‚æ•°å¢åŠ æ˜¯çº¿æ€§çš„ï¼ˆ1Ã—, 2Ã—, 3Ã—, ...ï¼‰
  è€Œåºåˆ—é•¿åº¦å¢åŠ æ˜¯äºŒæ¬¡çš„ï¼ˆ1Ã—, 4Ã—, 9Ã—, ...ï¼‰

å› æ­¤å¢åŠ å±‚æ•°æ¯”å¢åŠ å›¾åƒåˆ†è¾¨ç‡ä¾¿å®œå¾—å¤šï¼
```

---

### æ€»ç»“å¯¹æ¯”è¡¨

| æ”¹å˜ | Sequence Length (L) | Hidden Dim (D) | è®¡ç®—å¤æ‚åº¦ | æˆæœ¬å˜åŒ– |
|------|--------------------|--------------|-----------|---------| |
| **(i) ç»´åº¦ Ã— 2** | L | 2D | O(LÂ² Ã— 2D) | **2Ã—** |
| **(ii) å›¾åƒå°ºå¯¸ Ã— 2** | 4L | D | O(16LÂ² Ã— D) | **16Ã—** |
| **(iii) Patch å°ºå¯¸ Ã— 2** | L/4 | D | O(LÂ²/16 Ã— D) | **1/16Ã—** |
| **(iv) å±‚æ•° Ã— 2** | L | D | 2 Ã— O(LÂ² Ã— D) | **2Ã—** |

### å…³é”®æ´å¯Ÿ

```
è®¡ç®—æˆæœ¬æ’åºï¼ˆä»æœ€æ˜‚è´µåˆ°æœ€ä¾¿å®œï¼‰:
  1. å¢å¤§å›¾åƒå°ºå¯¸    â†’ 16Ã— âš ï¸ éå¸¸æ˜‚è´µï¼
  2. å¢åŠ ç»´åº¦        â†’ 2Ã—
  3. å¢åŠ å±‚æ•°        â†’ 2Ã—
  4. å¢å¤§ patch å°ºå¯¸ â†’ 1/16Ã— âœ“ å¾ˆä¾¿å®œï¼

å®é™…å»ºè®®:
  - éœ€è¦æå‡æ€§èƒ½: å¢åŠ å±‚æ•°æˆ–ç»´åº¦ï¼ˆæˆæœ¬é€‚ä¸­ï¼‰
  - éœ€è¦å¤„ç†é«˜åˆ†è¾¨ç‡: å¢å¤§ patch sizeï¼ˆé™ä½æˆæœ¬ï¼‰
  - é¿å…: ç›²ç›®å¢åŠ å›¾åƒåˆ†è¾¨ç‡ï¼ˆæˆæœ¬æš´æ¶¨ï¼‰
```

### è®¡ç®—ç¤ºä¾‹

å‡è®¾åŸºå‡†é…ç½®ï¼š
- å›¾åƒ: 32Ã—32
- Patch size: 8
- Hidden dim: 128
- Layers: 6

```
åŸºå‡†æˆæœ¬:
  L = (32/8)Â² = 16
  Cost_per_layer = O(16Â² Ã— 128) = O(32,768)
  Total_cost = 6 Ã— 32,768 = 196,608

åœºæ™¯å¯¹æ¯”:
(i)   Dâ†’256:     6 Ã— O(16Â² Ã— 256)    = 393,216    (2Ã—)
(ii)  64Ã—64:     6 Ã— O(64Â² Ã— 128)    = 3,145,728  (16Ã—)  âš ï¸
(iii) Patchâ†’16:  6 Ã— O(4Â² Ã— 128)     = 12,288     (1/16Ã—) âœ“
(iv)  Layersâ†’12: 12 Ã— O(16Â² Ã— 128)   = 393,216    (2Ã—)
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **ViT**: Dosovitskiy et al., "An Image is Worth 16x16 Words" (2020)
2. **DeiT**: Touvron et al., "Training data-efficient image transformers" (2021)
3. **Attention Is All You Need**: Vaswani et al. (2017)
4. **Data Augmentation**: Shorten & Khoshgoftaar, "A survey on Image Data Augmentation" (2019)

---

**æç¤º**: è¿™äº›ç­”æ¡ˆå¯ä»¥ç›´æ¥å¡«å…¥ notebook çš„å¯¹åº”ä½ç½®ï¼
