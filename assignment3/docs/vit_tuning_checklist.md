# Vision Transformer è°ƒä¼˜æ£€æŸ¥æ¸…å•

## âœ… å¼€å§‹å‰çš„å¿…åšæ£€æŸ¥

- [ ] **è¿‡æ‹Ÿåˆæµ‹è¯•**
  ```python
  # èƒ½åœ¨ 32 ä¸ªæ ·æœ¬ä¸Šè¾¾åˆ° 100% å‡†ç¡®ç‡å—ï¼Ÿ
  # å­¦ä¹ ç‡ï¼š5e-3, Dropout: 0.0, Weight Decay: 0.0
  # è®­ç»ƒ 150-200 æ­¥
  # âœ“ èƒ½ â†’ å®ç°æ­£ç¡®ï¼Œç»§ç»­
  # âœ— ä¸èƒ½ â†’ æœ‰ bugï¼Œå…ˆä¿®å¤
  ```

- [ ] **å½¢çŠ¶æ£€æŸ¥**
  ```python
  # è¾“å…¥ï¼š(N, 3, 32, 32)
  # Patches: (N, 16, embed_dim)  # å¯¹äº patch_size=8
  # è¾“å‡ºï¼š(N, num_classes)
  ```

- [ ] **æ¢¯åº¦æ£€æŸ¥**
  ```python
  # æ‰€æœ‰å‚æ•°éƒ½æœ‰æ¢¯åº¦å—ï¼Ÿ
  # æ¢¯åº¦èŒƒæ•°åœ¨åˆç†èŒƒå›´å†…å—ï¼Ÿ(0.1 ~ 10)
  ```

---

## ğŸ¯ åŸºç¡€é…ç½®ï¼ˆä»è¿™é‡Œå¼€å§‹ï¼‰

### æ¨¡å‹é…ç½®

```python
# å¿«é€Ÿå®éªŒï¼ˆæ¨èå¼€å§‹ï¼‰
VisionTransformer(
    img_size=32,
    patch_size=8,
    embed_dim=128,        # â† å°æ¨¡å‹
    num_layers=4,         # â† å°‘å±‚æ•°
    num_heads=4,
    dim_feedforward=512,
    dropout=0.1
)
# è®­ç»ƒæ—¶é—´ï¼š~10-15 åˆ†é’Ÿ/epoch (CIFAR-10)
```

### è®­ç»ƒé…ç½®

```python
# AdamW ä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-3,              # â† ä»è¿™ä¸ªå¼€å§‹
    weight_decay=1e-4     # â† è½»å¾®æ­£åˆ™åŒ–
)

# åŸºç¡€æ•°æ®å¢å¼º
transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))
])

# è®­ç»ƒè®¾ç½®
batch_size = 128      # å¦‚æœ OOMï¼Œé™åˆ° 64
num_epochs = 100
```

---

## ğŸ”§ è°ƒä¼˜æµç¨‹

### Step 1: å­¦ä¹ ç‡è°ƒä¼˜ï¼ˆæœ€é‡è¦ï¼ï¼‰

- [ ] **æµ‹è¯•ä¸åŒå­¦ä¹ ç‡**
  ```python
  # å°è¯•ï¼š[1e-2, 5e-3, 1e-3, 5e-4, 1e-4]
  # è§‚å¯Ÿï¼šloss æ›²çº¿
  #   - éœ‡è¡/NaN â†’ å¤ªå¤§
  #   - ä¸‹é™å¤ªæ…¢ â†’ å¤ªå°
  #   - ç¨³å®šä¸‹é™ â†’ âœ“
  ```

- [ ] **æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨**
  ```python
  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
      optimizer,
      T_max=num_epochs,
      eta_min=1e-6
  )
  ```

### Step 2: æ£€æŸ¥è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆ

**æ£€æŸ¥æŒ‡æ ‡**ï¼š
```
Train Acc: 0.95, Val Acc: 0.70  â†’ ä¸¥é‡è¿‡æ‹Ÿåˆ
Train Acc: 0.85, Val Acc: 0.75  â†’ è½»å¾®è¿‡æ‹Ÿåˆï¼ˆæ­£å¸¸ï¼‰
Train Acc: 0.60, Val Acc: 0.58  â†’ æ¬ æ‹Ÿåˆ
```

#### å¦‚æœè¿‡æ‹Ÿåˆï¼š

- [ ] **å¢åŠ  Dropout**
  ```python
  dropout=0.1 â†’ 0.2 æˆ– 0.3
  ```

- [ ] **å¢åŠ  Weight Decay**
  ```python
  weight_decay=1e-4 â†’ 1e-3
  ```

- [ ] **å¢å¼ºæ•°æ®å¢å¼º**
  ```python
  # æ·»åŠ  ColorJitter
  transforms.ColorJitter(0.2, 0.2, 0.2, 0.1)
  ```

- [ ] **å‡å°æ¨¡å‹**
  ```python
  embed_dim=256 â†’ 128
  num_layers=6 â†’ 4
  ```

#### å¦‚æœæ¬ æ‹Ÿåˆï¼š

- [ ] **å¢å¤§æ¨¡å‹**
  ```python
  embed_dim=128 â†’ 256 æˆ– 512
  num_layers=4 â†’ 6
  ```

- [ ] **é™ä½æ­£åˆ™åŒ–**
  ```python
  dropout=0.3 â†’ 0.1
  weight_decay=1e-3 â†’ 1e-4
  ```

- [ ] **è®­ç»ƒæ›´é•¿æ—¶é—´**
  ```python
  num_epochs=100 â†’ 200
  ```

### Step 3: æ·»åŠ è®­ç»ƒæŠ€å·§

- [ ] **æ¢¯åº¦è£å‰ª**
  ```python
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
  ```

- [ ] **Label Smoothing**
  ```python
  criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
  ```

- [ ] **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼ˆåŠ é€Ÿ 1.5-2xï¼‰
  ```python
  from torch.cuda.amp import autocast, GradScaler
  scaler = GradScaler()

  with autocast():
      output = model(x)
      loss = criterion(output, y)
  scaler.scale(loss).backward()
  scaler.step(optimizer)
  scaler.update()
  ```

- [ ] **Early Stopping**
  ```python
  patience = 15  # 15 epochs ä¸æå‡å°±åœæ­¢
  ```

---

## ğŸ“Š è¶…å‚æ•°é€ŸæŸ¥è¡¨

### å­¦ä¹ ç‡

| åœºæ™¯ | å­¦ä¹ ç‡ |
|------|--------|
| è¿‡æ‹Ÿåˆæµ‹è¯• | 5e-3 ~ 1e-2 |
| å°æ•°æ®é›† (<10K) | 1e-4 ~ 5e-4 |
| ä¸­ç­‰æ•°æ®é›† | 3e-4 ~ 1e-3 |
| å¤§æ•°æ®é›† (>100K) | 1e-3 ~ 5e-3 |

### Batch Size

| GPU | Batch Size |
|-----|-----------|
| 4GB | 32-64 |
| 8GB | 64-128 |
| 16GB+ | 128-256 |

### Weight Decay

| æ•°æ®é›† | Weight Decay |
|--------|-------------|
| è¿‡æ‹Ÿåˆæµ‹è¯• | 0.0 |
| å°æ•°æ®é›† | 1e-3 ~ 5e-3 |
| ä¸­ç­‰æ•°æ®é›† | 1e-4 ~ 1e-3 |
| å¤§æ•°æ®é›† | 1e-5 ~ 1e-4 |

### Dropout

| æ•°æ®é›† | Dropout |
|--------|---------|
| è¿‡æ‹Ÿåˆæµ‹è¯• | 0.0 |
| å°æ•°æ®é›† | 0.3 ~ 0.5 |
| ä¸­ç­‰æ•°æ®é›† | 0.1 ~ 0.3 |
| å¤§æ•°æ®é›† | 0.0 ~ 0.1 |

### æ¨¡å‹å¤§å°

| ç±»å‹ | Embed Dim | Layers | Heads | FFN Dim | å‚æ•°é‡ |
|------|-----------|--------|-------|---------|--------|
| Tiny | 128 | 4 | 4 | 512 | ~200K |
| Small | 256 | 6 | 8 | 1024 | ~2M |
| Base | 512 | 6 | 8 | 2048 | ~10M |

---

## ğŸ› å¸¸è§é—®é¢˜å¿«é€Ÿè¯Šæ–­

### Loss ä¸ä¸‹é™

```
âœ“ èƒ½è¿‡æ‹Ÿåˆä¸€ä¸ª batch?
  â†’ No: å®ç°æœ‰ bug
  â†’ Yes: å­¦ä¹ ç‡é—®é¢˜
    - å°è¯•æ›´å¤§çš„å­¦ä¹ ç‡ (1e-3 â†’ 5e-3)
    - å‡å°‘æ­£åˆ™åŒ–
```

### Loss å˜æˆ NaN

```
åŸå› ï¼šå­¦ä¹ ç‡å¤ªå¤§ æˆ– æ¢¯åº¦çˆ†ç‚¸
è§£å†³ï¼š
  1. é™ä½å­¦ä¹ ç‡ (1e-3 â†’ 1e-4)
  2. æ·»åŠ æ¢¯åº¦è£å‰ª (max_norm=1.0)
  3. æ£€æŸ¥è¾“å…¥æ˜¯å¦æ ‡å‡†åŒ–
```

### GPU å†…å­˜ä¸è¶³

```
è§£å†³æ–¹æ¡ˆï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
  1. å‡å° batch_size (128 â†’ 64 â†’ 32)
  2. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (èŠ‚çœ ~50% å†…å­˜)
  3. å‡å°æ¨¡å‹ (embed_dim=256 â†’ 128)
  4. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
```

### è®­ç»ƒå¤ªæ…¢

```
åŠ é€Ÿæ–¹æ³•ï¼š
  1. æ··åˆç²¾åº¦è®­ç»ƒ (1.5-2x)
  2. å¢å¤§ batch_size
  3. ä½¿ç”¨ pin_memory=True
  4. å¢åŠ  num_workers
  5. å‡å°æ¨¡å‹æˆ–å±‚æ•°
```

### éªŒè¯é›†å‡†ç¡®ç‡è¿œä½äºè®­ç»ƒé›†

```
è¿‡æ‹Ÿåˆï¼
  1. å¢å¤§ dropout (0.1 â†’ 0.2)
  2. å¢å¤§ weight_decay (1e-4 â†’ 1e-3)
  3. å¢åŠ æ•°æ®å¢å¼º
  4. å‡å°æ¨¡å‹å®¹é‡
```

---

## ğŸ“ˆ è®­ç»ƒç›‘æ§æŒ‡æ ‡

### æ¯ä¸ª Epoch åº”è¯¥è®°å½•

- [ ] Train Loss
- [ ] Train Accuracy
- [ ] Val Loss
- [ ] Val Accuracy
- [ ] Learning Rate
- [ ] Gradient Normï¼ˆå¯é€‰ï¼‰

### åˆ¤æ–­æ ‡å‡†

```python
# æ­£å¸¸è®­ç»ƒ
Train Loss: æŒç»­ä¸‹é™
Val Loss: å…ˆä¸‹é™ï¼Œåç¨å¾®ä¸Šå‡ï¼ˆè½»å¾®è¿‡æ‹Ÿåˆæ­£å¸¸ï¼‰
Train Acc: æŒç»­ä¸Šå‡
Val Acc: ä¸Šå‡åç¨³å®š

# å¼‚å¸¸æƒ…å†µ
Loss éœ‡è¡ â†’ å­¦ä¹ ç‡å¤ªå¤§
Loss ä¸å˜ â†’ å­¦ä¹ ç‡å¤ªå° æˆ– å®ç°é”™è¯¯
Val Loss æ€¥å‰§ä¸Šå‡ â†’ ä¸¥é‡è¿‡æ‹Ÿåˆ
NaN â†’ æ¢¯åº¦çˆ†ç‚¸ æˆ– æ•°å€¼ä¸ç¨³å®š
```

---

## ğŸ¯ æ¨èè°ƒä¼˜é¡ºåº

### ç¬¬ä¸€è½®ï¼šåŸºç¡€é…ç½®

1. [ ] è¿‡æ‹Ÿåˆæµ‹è¯•ï¼ˆéªŒè¯å®ç°ï¼‰
2. [ ] é€‰æ‹©å°æ¨¡å‹ï¼ˆå¿«é€Ÿè¿­ä»£ï¼‰
3. [ ] è°ƒæ•´å­¦ä¹ ç‡ï¼ˆæœ€é‡è¦ï¼‰
4. [ ] åŸºç¡€æ•°æ®å¢å¼º

**ç›®æ ‡**ï¼šè¾¾åˆ°åˆç†çš„åŸºçº¿æ€§èƒ½ï¼ˆ~60-70% val accï¼‰

### ç¬¬äºŒè½®ï¼šå‡å°‘è¿‡æ‹Ÿåˆ

5. [ ] è°ƒæ•´ dropout å’Œ weight decay
6. [ ] å¢åŠ æ•°æ®å¢å¼º
7. [ ] æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨

**ç›®æ ‡**ï¼šç¼©å° train-val gap

### ç¬¬ä¸‰è½®ï¼šæå‡æ€§èƒ½

8. [ ] å¢å¤§æ¨¡å‹ï¼ˆå¦‚æœæœ‰å¿…è¦ï¼‰
9. [ ] Label smoothing
10. [ ] æ¢¯åº¦è£å‰ª
11. [ ] Warmupï¼ˆå¤§æ¨¡å‹ï¼‰

**ç›®æ ‡**ï¼šè¾¾åˆ°æœ€ä¼˜æ€§èƒ½

### ç¬¬å››è½®ï¼šä¼˜åŒ–æ•ˆç‡

12. [ ] æ··åˆç²¾åº¦è®­ç»ƒ
13. [ ] è°ƒæ•´ batch size
14. [ ] Early stopping

**ç›®æ ‡**ï¼šæ›´å¿«çš„è®­ç»ƒé€Ÿåº¦

---

## ğŸ’¾ å®Œæ•´è®­ç»ƒæ¨¡æ¿

```python
# ===== é…ç½® =====
config = {
    # æ¨¡å‹
    'embed_dim': 128,
    'num_layers': 4,
    'num_heads': 4,
    'dropout': 0.1,

    # è®­ç»ƒ
    'batch_size': 128,
    'num_epochs': 100,
    'lr': 1e-3,
    'weight_decay': 1e-4,
    'grad_clip': 1.0,

    # å…¶ä»–
    'label_smoothing': 0.1,
    'warmup_epochs': 5,
}

# ===== æ¨¡å‹ =====
model = VisionTransformer(
    img_size=32,
    patch_size=8,
    in_channels=3,
    embed_dim=config['embed_dim'],
    num_layers=config['num_layers'],
    num_heads=config['num_heads'],
    dim_feedforward=config['embed_dim'] * 4,
    num_classes=10,
    dropout=config['dropout']
).cuda()

# ===== ä¼˜åŒ–å™¨ =====
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=config['lr'],
    weight_decay=config['weight_decay']
)

# ===== å­¦ä¹ ç‡è°ƒåº¦ =====
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=config['num_epochs']
)

# ===== æŸå¤±å‡½æ•° =====
criterion = nn.CrossEntropyLoss(
    label_smoothing=config['label_smoothing']
)

# ===== è®­ç»ƒå¾ªç¯ =====
for epoch in range(config['num_epochs']):
    # è®­ç»ƒ
    model.train()
    for x, y in train_loader:
        x, y = x.cuda(), y.cuda()

        optimizer.zero_grad()
        output = model(x)
        loss = criterion(output, y)
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(
            model.parameters(),
            max_norm=config['grad_clip']
        )

        optimizer.step()

    # éªŒè¯
    model.eval()
    # ... validation code ...

    # æ›´æ–°å­¦ä¹ ç‡
    scheduler.step()
```

---

## ğŸ“ ç»éªŒæ€»ç»“

### ä¸€å®šè¦åš âœ…
1. è¿‡æ‹Ÿåˆæµ‹è¯•ï¼ˆéªŒè¯å®ç°æ­£ç¡®ï¼‰
2. è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå½±å“æœ€å¤§ï¼‰
3. æ•°æ®å¢å¼ºï¼ˆViT å¿…éœ€ï¼‰
4. ç›‘æ§è®­ç»ƒæ›²çº¿

### æ¨èåš â­
5. å­¦ä¹ ç‡è°ƒåº¦å™¨
6. æ¢¯åº¦è£å‰ª
7. Label smoothing
8. æ··åˆç²¾åº¦ï¼ˆåŠ é€Ÿï¼‰

### å¯é€‰åš ğŸ’¡
9. Warmupï¼ˆå¤§æ¨¡å‹ï¼‰
10. Mixup/Cutmix
11. Stochastic depth
12. è¶…å‚æ•°æœç´¢

### å¸¸è§é”™è¯¯ âŒ
- æ²¡åšè¿‡æ‹Ÿåˆæµ‹è¯•å°±å¼€å§‹è®­ç»ƒ
- å­¦ä¹ ç‡è®¾ç½®ä¸å½“
- ViT ä¸ç”¨æ•°æ®å¢å¼º
- è¿‡åº¦æ­£åˆ™åŒ–å¯¼è‡´æ¬ æ‹Ÿåˆ
- æ²¡æœ‰ç›‘æ§è®­ç»ƒè¿‡ç¨‹

---

**è®°ä½**ï¼šä»ç®€å•å¼€å§‹ï¼Œé€æ­¥æ·»åŠ å¤æ‚æŠ€å·§ã€‚æ¯æ¬¡åªæ”¹å˜ä¸€ä¸ªè¶…å‚æ•°ï¼Œè§‚å¯Ÿæ•ˆæœï¼

ğŸ“– è¯¦ç»†æ–‡æ¡£ï¼š`vit_tuning_guide.md`
